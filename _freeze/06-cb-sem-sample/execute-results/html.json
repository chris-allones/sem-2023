{
  "hash": "3c697a12ffa9103a6ac10e4117eff13c",
  "result": {
    "markdown": "---\ntitle: \"Covariance Based SEM (CB-SEM)\"\nbibliography: references.bib\n\n---\n\n::: {.cell}\n\n:::\n\n\n## Sample study\n\n-   Journal article: [Young people's perceived service quality and environmental performance of hybrid electric bus.](https://doi.org/10.1016/j.tbs.2020.03.003)\n-   Author: Zial Haque and Tehjeeb Noor\n-   Article link: [DOI link](https://doi.org/10.1016/j.tbs.2020.03.003)\n-   Download the dataset [here](https://ars.els-cdn.com/content/image/1-s2.0-S2214367X19302492-mmc1.xlsx).\n\n\n![](plots/sample_study.png){fig-align=\"center\"}\n\n\n## Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Library\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(janitor)\nlibrary(lavaan)\nlibrary(psych)\nlibrary(MVN)\nlibrary(semTools)\nlibrary(lavaanPlot)\n```\n:::\n\n\n## Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## data\ncase_data <- read_excel(\"00_data/e_bus_customer_satisfaction.xlsx\") %>% \n clean_names()\n\ncase_data_items <- case_data %>%\n select(bt1:bt7, bd1:bd4, emp1:emp5, cs1:cs3, ep1:ep4, ls1:ls5)\n\n## datatable\nDT::datatable(case_data)\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"datatables html-widget html-fill-item-overflow-hidden html-fill-item\" id=\"htmlwidget-7d2ae0b1bc23be173598\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-7d2ae0b1bc23be173598\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"132\",\"133\",\"134\",\"135\",\"136\",\"137\",\"138\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"145\",\"146\",\"147\",\"148\",\"149\",\"150\",\"151\",\"152\",\"153\",\"154\",\"155\",\"156\",\"157\",\"158\",\"159\",\"160\",\"161\",\"162\",\"163\",\"164\",\"165\",\"166\",\"167\",\"168\",\"169\",\"170\",\"171\",\"172\",\"173\",\"174\",\"175\",\"176\",\"177\",\"178\",\"179\",\"180\",\"181\",\"182\",\"183\",\"184\",\"185\",\"186\",\"187\",\"188\",\"189\",\"190\",\"191\",\"192\",\"193\",\"194\",\"195\",\"196\",\"197\",\"198\",\"199\",\"200\",\"201\",\"202\",\"203\",\"204\",\"205\",\"206\",\"207\",\"208\",\"209\",\"210\",\"211\",\"212\",\"213\",\"214\",\"215\",\"216\",\"217\",\"218\",\"219\",\"220\",\"221\",\"222\",\"223\",\"224\",\"225\",\"226\",\"227\",\"228\",\"229\",\"230\",\"231\",\"232\",\"233\",\"234\",\"235\",\"236\",\"237\",\"238\",\"239\",\"240\",\"241\",\"242\",\"243\",\"244\",\"245\",\"246\",\"247\",\"248\",\"249\",\"250\",\"251\",\"252\",\"253\",\"254\",\"255\",\"256\",\"257\",\"258\",\"259\",\"260\",\"261\",\"262\",\"263\",\"264\",\"265\",\"266\",\"267\",\"268\",\"269\",\"270\",\"271\",\"272\"],[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272],[4,1,1,4,1,4,1,1,4,1,2,3,4,1,1,3,1,1,3,4,1,3,4,2,4,2,1,4,1,4,1,4,4,4,4,1,2,4,1,1,2,1,1,4,1,4,4,4,1,4,4,4,1,1,1,1,1,4,1,1,1,4,4,1,2,3,1,4,3,1,4,1,1,1,4,1,3,1,1,1,4,2,1,3,3,1,4,4,4,1,1,3,4,4,3,4,4,2,2,1,4,4,4,4,2,2,1,2,1,4,4,1,1,1,4,4,1,2,1,1,1,4,1,2,4,4,1,1,3,4,1,1,1,4,1,4,2,4,1,1,4,3,2,2,4,4,4,1,4,4,1,4,4,4,4,4,1,2,3,4,1,4,1,4,1,1,4,2,4,4,1,1,2,4,4,1,1,4,1,2,2,4,4,4,4,4,4,1,1,4,1,2,4,4,4,4,1,4,4,1,1,3,1,4,1,3,4,4,1,4,1,4,1,4,1,4,4,1,1,4,4,1,4,1,1,1,4,2,1,4,4,4,4,4,1,4,4,4,4,4,4,4,4,4,4,2,1,1,4,2,1,2,4,1,4,1,1,4,1,2,4,4,4,4,4,4,1,2,1,4,1,1],[26,24,22,33,23,23,25,23,22,25,23,21,23,23,24,23,22,24,22,26,24,22,25,22,25,23,22,24,23,23,26,23,25,25,24,23,22,23,34,24,23,23,25,25,26,30,32,26,22,22,21,20,20,21,22,19,20,20,22,20,22,24,21,22,21,23,20,21,25,21,23,21,24,23,28,23,22,31,24,21,20,21,22,20,23,21,22,20,21,22,29,25,21,21,27,20,21,22,20,22,22,19,19,20,22,20,22,19,20,19,21,24,21,20,19,27,20,26,23,29,20,34,20,20,23,19,21,19,20,19,20,20,21,21,19,20,19,19,20,19,20,22,21,19,22,23,19,19,22,21,26,21,19,19,19,22,19,19,20,20,23,19,21,20,19,19,21,19,21,21,23,19,19,20,19,21,20,21,19,19,20,23,19,19,21,19,21,20,20,27,19,23,21,20,19,19,19,20,22,20,21,21,20,19,20,21,22,26,22,19,21,19,27,23,21,30,24,20,19,22,21,20,19,21,32,21,22,20,20,20,20,21,19,21,20,20,19,21,20,21,19,19,20,20,21,19,19,20,21,20,19,18,23,19,32,20,21,20,30,19,20,20,21,19,19,19,19,20,23,20,21,22],[0,1,0,1,0,0,0,0,0,1,1,0,0,1,0,1,1,0,1,0,0,1,0,1,1,1,0,0,0,0,1,1,0,0,0,1,0,0,1,0,1,0,1,0,0,1,1,0,0,1,1,1,1,0,0,1,0,1,1,1,0,0,0,0,0,0,1,0,0,1,0,1,0,1,1,0,1,0,0,1,1,1,0,1,0,1,1,1,0,1,0,0,0,0,0,1,0,0,1,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,1,1,1,1,1,1,0,0,1,0,1,0,0,1,0,1,0,0,0,1,1,1,1,1,0,1,1,0,1,1,0,1,1,1,0,1,1,0,0,1,1,1,1,1,1,0,1,1,0,1,0,1,1,0,1,0,0,1,1,1,0,0,1,1,1,0,0,0,0,1,1,1,0,1,1,1,1,1,0,1,1,0,1,0,1,1,1,0,1,1,0,1,1,1,1,0,1,1,0,1,0,1,0,1,1,0,0,1,0,1,1,0,1,1,1,0,1,0,1,1,1,1,1,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,1,0,1,0,0,1,0,0,1,1,1,0,1],[1,5,3,1,1,3,1,1,1,1,4,3,3,5,5,5,4,4,1,1,5,4,5,4,1,5,1,1,2,1,1,1,3,1,1,4,2,1,1,2,4,3,4,4,1,1,5,2,1,1,1,1,1,2,1,1,1,1,1,1,1,2,5,5,1,2,4,2,4,5,1,5,5,1,1,3,1,5,5,2,1,3,5,2,1,1,1,1,4,2,1,2,1,1,1,2,1,1,1,1,1,2,1,1,3,4,1,1,1,2,1,1,1,1,1,5,1,1,5,1,5,1,5,1,1,1,5,1,1,4,2,5,1,4,4,1,3,3,5,1,3,1,5,1,1,1,3,1,1,1,1,1,1,2,2,1,1,2,5,1,5,1,1,1,1,4,2,2,1,2,1,5,3,1,5,1,4,1,1,3,3,4,1,1,1,1,1,5,1,2,1,1,2,1,1,1,1,1,2,1,5,1,4,2,3,2,2,2,1,2,1,1,1,2,4,1,3,1,1,4,1,5,1,1,1,1,3,1,3,1,1,1,4,2,5,1,1,3,1,1,1,1,3,1,1,3,1,5,3,1,1,1,1,2,1,2,1,3,1,1,4,1,1,3,2,3,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,1,2,5,1,1,1,1,2,1,1,1,1,1,1,1,1,4,2,1,1,1,1,1,1,1,1,1,1,1,1,5,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,4,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,4,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,1,1,1,1,1,1,1,1,1,3,1,1,1,1,1,1,1,1,1,1,1,1,5,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,3,1,1,1],[1,2,1,3,5,1,4,1,4,5,1,1,1,3,1,2,2,2,1,5,3,1,1,1,3,1,3,1,5,3,5,1,1,4,2,4,1,1,4,1,3,5,4,1,5,3,1,1,2,1,1,2,4,2,3,5,5,1,2,3,4,3,5,1,5,2,1,1,3,3,4,1,4,1,3,4,4,4,1,4,1,1,1,1,1,2,1,2,1,5,5,1,1,2,2,1,3,1,2,3,1,3,3,4,1,3,5,3,5,1,3,5,4,5,1,1,5,1,2,1,2,2,1,1,2,4,1,1,5,2,1,3,1,2,5,5,1,1,1,5,1,4,1,1,2,1,1,1,2,1,4,1,3,4,2,3,4,1,2,2,5,4,5,1,5,1,3,1,1,3,5,1,4,4,5,3,2,1,1,1,1,1,4,4,1,4,4,1,5,1,1,5,3,3,1,3,4,2,2,5,1,3,2,1,4,1,1,1,4,1,5,1,5,1,4,1,4,5,1,1,1,1,3,2,5,1,3,1,1,4,1,2,2,2,1,2,1,1,3,4,4,4,3,1,2,1,5,1,1,2,1,1,3,5,4,2,1,2,1,3,4,4,1,2,2,1,2,1,5,1,4,1],[5,2,1,5,1,5,2,1,5,2,4,3,4,2,2,4,1,2,3,5,3,4,5,3,5,2,3,5,1,3,1,5,4,5,5,3,2,3,1,2,3,3,3,5,2,5,5,3,1,5,5,5,1,3,2,2,2,5,2,2,2,4,5,3,2,4,2,4,3,2,4,2,2,3,5,2,3,2,2,1,5,2,2,5,4,2,5,5,4,2,1,3,5,5,5,5,4,1,3,1,5,4,5,4,3,3,1,4,2,5,4,1,2,1,4,5,1,3,1,4,1,5,3,2,5,4,2,2,3,5,1,1,1,5,2,3,3,4,2,2,5,3,3,4,5,3,5,3,4,4,1,5,5,5,3,3,2,3,2,3,2,4,1,5,3,1,4,4,5,5,2,1,2,5,5,1,1,5,1,4,3,5,5,5,5,4,4,2,2,5,2,5,5,3,5,5,2,5,5,1,2,2,2,5,2,3,4,5,1,5,1,5,3,5,2,4,5,2,2,4,5,1,3,2,3,2,5,2,3,5,5,5,4,5,1,4,4,5,3,4,4,4,3,5,5,5,3,1,5,4,3,3,5,2,3,1,2,4,1,2,4,5,5,4,4,5,2,2,2,5,2,1],[4,2,5,4,3,3,5,5,2,4,4,5,3,5,4,3,5,3,3,4,3,5,5,5,3,2,4,4,2,3,3,5,3,1,3,5,5,4,2,5,4,2,3,4,3,2,5,5,2,5,5,4,3,5,5,2,3,3,5,3,3,3,5,4,2,3,4,4,5,2,2,3,3,5,3,4,2,4,3,5,5,3,4,5,4,5,4,5,5,4,2,5,3,3,4,4,3,5,5,1,4,3,4,4,5,5,2,2,2,3,2,1,3,3,1,1,1,5,3,3,3,4,5,5,2,2,4,5,2,3,5,2,5,5,4,3,5,4,2,3,4,4,5,5,5,5,4,3,2,5,3,3,5,5,2,1,4,5,2,4,5,3,3,4,2,5,3,5,2,4,2,5,3,2,5,5,2,5,5,5,3,4,4,4,4,3,2,2,5,4,5,3,3,3,5,4,3,5,5,1,5,2,3,5,5,3,5,1,3,5,2,4,2,4,5,4,3,1,5,4,5,5,3,5,1,5,5,5,5,4,3,3,3,3,4,3,5,4,1,3,2,3,2,3,3,4,2,1,3,4,5,5,5,3,2,5,5,2,5,3,2,3,4,2,5,5,5,5,2,2,3,5],[5,4,3,7,6,5,5,4,5,4,5,5,7,6,4,3,3,6,3,6,5,4,7,6,2,5,7,3,4,6,5,4,5,4,7,5,6,6,5,4,3,1,5,7,3,6,6,4,4,6,6,5,3,2,7,4,5,6,6,2,4,5,7,4,5,5,4,5,7,5,5,6,6,5,6,5,5,3,5,5,5,2,4,4,6,6,6,6,6,3,3,5,6,6,4,5,5,5,4,7,5,5,4,3,4,5,6,6,6,3,4,1,4,2,1,7,4,3,7,6,6,5,6,4,7,5,3,3,6,6,3,4,6,6,5,6,6,4,6,4,5,6,7,7,6,5,6,6,4,4,6,4,3,5,4,5,5,5,5,5,7,5,6,5,4,4,6,5,7,4,3,4,6,4,7,4,5,7,5,6,3,4,7,2,7,6,5,6,5,6,4,4,4,6,5,6,5,5,5,3,4,3,4,5,5,4,5,7,5,6,6,3,4,6,6,2,5,6,5,4,6,7,4,6,4,4,4,6,3,5,4,4,5,6,5,4,5,4,4,4,7,7,6,4,6,6,4,4,3,5,7,5,4,5,4,6,5,6,7,2,4,6,5,5,3,5,6,6,6,6,4,5],[5,5,3,7,5,5,6,4,5,4,5,4,5,4,5,3,3,6,2,4,5,4,7,6,6,5,4,4,4,6,7,6,6,6,7,5,4,6,5,6,5,4,6,7,3,5,6,5,4,6,6,5,4,4,5,3,5,5,6,2,6,4,7,6,5,6,4,5,6,6,6,6,6,5,5,5,6,3,5,6,5,5,4,4,6,6,6,6,6,3,4,5,7,5,5,7,5,5,3,7,6,5,4,3,6,4,3,7,6,6,3,1,3,3,1,7,4,6,6,6,6,7,6,2,7,5,3,2,5,6,2,4,6,5,4,5,4,4,6,3,6,6,7,7,6,4,6,6,3,2,6,4,3,5,3,5,5,4,4,5,7,5,6,6,5,4,6,5,5,5,1,6,6,2,7,5,5,4,3,5,3,5,7,6,7,4,5,6,5,6,5,3,3,7,6,7,5,4,5,4,4,2,2,5,5,4,5,4,3,5,5,6,5,6,5,5,4,6,6,4,6,7,4,5,1,6,4,6,3,6,3,4,5,6,5,4,4,3,6,5,5,5,6,3,3,6,5,4,4,6,7,4,4,5,4,6,5,6,6,4,3,3,5,6,3,5,6,6,5,6,3,5],[4,4,2,7,6,3,7,4,6,4,4,4,4,5,6,4,4,6,5,2,4,2,7,6,4,5,3,1,4,2,5,4,5,4,3,5,4,5,4,5,4,2,6,6,6,5,4,4,5,6,7,6,3,6,7,3,3,5,6,2,5,6,7,5,5,5,1,1,5,6,4,6,6,4,6,4,5,4,5,2,6,6,6,4,6,5,6,6,2,3,3,4,6,5,6,5,2,4,5,7,3,5,4,3,4,3,4,3,7,3,3,5,4,4,1,3,7,6,7,6,3,6,2,2,7,3,3,2,4,5,5,5,6,5,5,5,7,5,5,6,7,4,7,7,4,4,4,6,3,5,6,2,3,4,6,4,5,5,6,4,6,4,6,5,5,4,6,6,3,6,1,6,4,6,7,3,6,6,3,5,3,4,2,3,6,2,3,5,6,5,5,6,3,7,6,4,6,4,4,3,3,2,3,5,4,3,6,6,6,3,6,4,5,6,7,6,4,4,5,2,5,7,2,6,2,4,3,6,4,2,3,5,6,6,6,3,2,3,3,3,4,4,4,2,2,6,7,7,4,5,7,6,6,2,4,6,2,5,6,3,2,6,7,5,3,2,6,6,3,6,4,7],[6,4,2,7,7,5,5,4,6,4,3,4,5,5,3,5,5,6,4,6,4,4,7,6,3,4,7,4,4,5,4,6,6,5,5,5,5,3,5,5,4,3,6,6,5,7,6,4,4,6,4,6,4,5,7,3,1,5,6,2,6,4,7,5,5,5,3,6,6,6,4,6,6,3,5,4,4,3,4,3,5,4,2,3,6,6,6,6,4,3,3,4,7,5,7,5,2,5,3,4,5,5,4,3,5,4,2,6,7,5,3,1,3,3,4,3,4,5,7,6,4,4,4,3,7,4,3,5,6,5,3,4,6,6,4,3,5,6,5,6,7,5,5,7,4,5,4,6,7,4,6,2,7,4,3,6,5,5,6,5,6,4,6,6,4,7,5,4,5,4,3,6,5,7,4,5,6,4,4,6,3,6,7,6,6,4,2,5,5,5,4,4,3,7,7,7,6,4,5,4,4,2,4,5,4,4,5,4,2,4,6,7,5,5,5,5,6,5,4,4,4,7,3,4,2,5,3,6,4,6,5,4,7,6,5,4,4,2,4,5,7,6,6,4,6,7,3,4,4,6,7,5,5,6,4,6,3,6,5,2,3,6,7,3,4,5,6,5,5,4,3,7],[6,4,3,7,6,5,5,3,7,4,4,5,6,5,4,4,4,5,3,3,4,4,7,6,4,6,6,5,4,5,3,6,3,5,6,5,5,3,4,5,4,5,5,6,4,6,2,3,3,4,4,5,3,3,6,4,3,5,6,2,5,6,7,3,5,5,3,4,5,6,4,6,6,3,7,4,7,4,4,4,5,4,6,4,6,5,6,6,1,3,3,2,7,5,2,6,3,5,4,4,5,5,4,3,3,5,2,7,6,2,4,1,3,4,3,3,4,4,7,6,4,6,4,3,4,3,3,4,4,7,3,3,6,6,6,4,6,5,4,3,6,5,4,7,6,3,4,5,4,3,6,3,1,4,3,5,5,7,6,5,6,4,6,5,5,4,7,5,6,5,4,3,5,4,4,6,4,2,3,5,3,1,5,5,7,4,5,4,6,5,5,4,4,7,6,7,6,5,5,5,4,3,4,5,5,5,4,3,3,3,6,3,3,6,6,4,3,5,4,5,4,7,4,6,2,5,5,6,5,6,3,4,3,3,4,4,2,3,4,4,7,7,6,2,4,6,5,4,5,6,7,4,3,6,4,6,3,6,6,4,2,6,5,7,4,1,4,7,5,7,3,7],[6,4,3,7,7,6,4,3,7,4,5,6,5,4,6,3,5,6,4,7,4,7,6,6,4,4,7,4,4,4,7,5,2,7,6,6,6,6,4,7,5,7,7,6,5,7,6,4,2,2,6,5,4,4,7,4,7,6,6,2,4,5,7,5,5,5,3,6,6,4,6,6,4,3,6,6,7,3,5,2,6,5,6,5,5,6,6,6,3,3,4,6,7,6,6,6,6,5,3,7,6,4,4,3,4,5,2,7,7,2,5,1,4,2,4,3,7,2,6,6,4,3,6,2,4,3,3,5,5,7,6,6,6,5,6,6,7,4,6,4,6,7,6,7,4,6,3,6,6,4,6,4,5,4,5,7,7,5,6,6,6,6,6,6,5,4,7,4,6,7,4,2,4,4,7,6,5,4,4,7,3,5,7,6,7,6,7,6,5,6,2,5,4,7,7,7,7,5,4,3,4,5,3,4,5,5,6,7,5,6,6,7,5,6,4,3,5,6,3,4,6,7,5,5,1,6,4,6,5,7,5,5,7,5,6,4,4,5,6,3,7,7,4,4,6,7,6,6,5,6,7,6,4,7,4,5,5,5,7,6,4,5,4,6,5,6,6,6,7,6,4,7],[7,4,2,7,5,4,4,3,6,3,5,6,4,5,6,3,5,6,4,7,4,7,7,6,5,5,5,5,4,6,6,4,3,6,7,6,6,4,4,7,5,6,7,7,6,7,6,5,3,5,6,5,3,6,6,4,5,6,6,2,5,5,5,5,5,5,3,5,7,6,5,6,5,3,7,5,6,5,3,4,6,4,5,5,6,6,6,6,2,3,7,5,7,5,4,7,6,3,4,7,7,3,4,3,4,5,6,7,7,1,4,1,3,1,4,3,7,2,7,6,4,6,5,2,6,5,2,3,7,5,7,2,6,3,7,7,7,5,6,6,5,7,3,7,3,4,6,3,4,2,6,3,1,4,6,6,7,6,4,7,7,5,6,5,5,5,7,7,3,6,5,1,4,4,7,4,5,5,4,7,3,6,7,7,7,5,7,6,6,6,5,6,4,7,6,4,7,7,4,5,4,5,4,4,5,6,3,7,6,6,6,7,5,7,6,7,6,7,4,4,6,7,5,6,1,5,5,6,4,7,5,5,7,6,4,4,5,5,5,4,7,7,6,2,6,6,6,7,5,4,7,6,4,7,4,5,5,6,6,4,4,5,5,6,5,6,4,6,5,5,3,7],[6,4,3,4,4,5,3,4,5,4,3,5,6,5,5,4,4,6,3,4,6,4,7,6,1,5,6,2,4,4,6,6,5,6,3,4,5,6,5,4,3,6,6,5,3,7,4,6,4,1,1,4,2,5,5,4,2,5,5,1,5,3,1,7,6,5,3,1,7,4,6,2,7,4,4,5,6,4,6,3,6,4,2,3,5,6,4,6,1,3,3,3,7,6,6,4,1,6,4,5,6,4,6,3,7,5,7,6,6,5,3,2,3,2,4,3,5,2,6,3,4,4,2,7,4,4,4,2,2,4,4,4,6,4,4,6,5,6,6,6,6,6,5,6,4,5,4,6,4,3,6,5,7,5,3,4,6,7,6,5,5,4,5,7,3,7,4,5,4,4,2,6,5,5,5,6,6,4,5,6,3,5,7,7,6,5,5,6,7,4,5,5,4,7,7,6,7,6,3,3,4,3,6,4,5,5,4,2,5,2,6,5,5,4,4,5,6,4,6,1,5,7,5,6,1,5,2,6,5,5,5,3,4,6,6,3,4,4,6,5,5,5,5,3,3,6,5,7,4,7,7,3,4,4,4,6,3,5,7,3,5,5,5,5,2,5,5,4,4,4,3,6],[6,4,3,4,6,5,3,4,6,4,6,5,6,5,5,2,6,5,3,6,6,6,7,6,4,5,7,5,4,6,6,6,6,7,7,4,6,7,4,7,4,6,4,6,6,7,4,6,4,2,3,5,2,5,4,4,6,5,6,1,1,2,7,5,6,5,3,3,7,5,4,3,6,5,6,5,6,5,5,2,5,4,5,4,5,5,4,7,2,4,7,3,7,6,6,5,3,5,4,7,6,3,6,3,4,5,7,6,7,2,3,1,2,3,4,3,5,6,6,4,4,6,4,7,6,3,4,5,6,7,6,4,6,5,3,6,6,6,7,6,5,6,5,7,5,5,7,7,6,3,6,6,7,5,4,6,5,7,6,5,6,5,6,6,4,7,5,5,7,4,3,2,5,7,7,5,6,5,5,7,3,5,6,6,6,6,6,7,7,5,5,6,4,7,7,6,6,5,3,5,4,3,6,4,5,4,3,7,4,4,6,6,4,5,6,7,6,6,6,2,6,7,6,7,3,6,5,6,5,6,5,5,7,5,6,3,3,4,7,5,6,6,6,5,7,6,5,7,5,5,7,4,4,6,4,6,5,6,6,5,5,5,5,7,3,6,4,6,6,7,2,6],[6,5,3,5,5,4,2,4,4,4,3,5,6,5,5,4,3,6,2,7,5,5,7,3,1,4,5,5,4,4,4,6,4,5,4,4,6,6,4,7,5,2,5,4,5,6,3,4,4,3,3,4,3,1,3,4,3,4,6,2,1,3,3,6,5,5,3,3,7,5,5,3,6,4,5,4,6,3,4,3,6,2,3,3,5,6,6,6,3,4,3,4,6,7,6,4,1,5,4,4,3,4,6,3,5,4,6,4,5,4,3,2,2,1,4,3,5,2,7,5,4,3,3,6,5,4,5,1,4,6,5,3,6,6,4,6,5,6,4,6,5,5,6,4,4,7,6,5,3,2,6,4,7,5,3,6,6,6,5,4,4,5,6,7,4,7,7,5,6,3,2,7,5,5,7,5,6,4,4,6,2,4,7,6,6,5,6,4,6,5,5,5,2,7,7,7,4,5,4,4,4,3,6,6,5,5,2,3,3,3,4,6,5,4,6,6,6,3,6,2,5,7,5,7,2,6,4,4,4,6,4,3,4,6,5,3,2,3,7,6,6,5,6,2,4,5,7,7,5,3,6,4,2,3,4,6,4,3,6,2,5,3,5,5,3,4,4,6,6,6,2,6],[6,4,3,4,5,4,3,3,5,4,5,5,4,4,6,6,5,6,4,6,4,5,7,4,4,3,5,4,4,4,3,6,4,4,5,4,5,4,4,6,4,5,5,4,5,7,2,4,3,1,2,4,3,4,6,4,5,4,6,2,6,5,7,7,5,5,3,6,7,4,4,2,5,3,6,5,6,3,4,4,5,4,4,4,5,6,4,6,2,3,7,4,7,6,3,3,2,6,4,5,6,3,6,3,6,4,4,7,6,4,4,1,2,1,4,3,5,4,6,4,4,4,4,6,5,4,4,4,4,6,5,3,6,5,5,6,5,5,4,6,5,4,4,7,4,5,5,3,4,2,6,5,7,5,4,5,6,7,6,4,6,5,6,7,4,7,5,5,6,3,3,6,5,7,5,5,6,3,4,7,3,3,7,4,6,4,7,4,6,5,5,4,4,7,5,7,5,5,3,4,4,4,6,4,5,4,4,5,4,4,5,7,4,6,5,4,4,6,6,3,5,7,5,5,2,5,5,5,5,6,4,4,4,5,6,3,5,3,6,4,7,6,6,2,6,6,4,7,5,3,7,3,4,2,4,5,7,6,4,4,4,3,5,7,3,4,4,5,5,5,2,6],[4,6,2,3,6,6,1,3,7,5,6,7,6,3,4,6,2,7,4,7,5,7,7,3,7,6,7,3,4,5,7,6,3,7,6,4,6,4,4,6,7,7,7,1,4,7,6,6,3,6,2,4,3,6,6,5,7,4,3,3,4,6,1,6,6,4,4,7,3,6,3,3,1,5,4,7,7,5,6,2,3,7,4,5,6,6,4,6,6,4,2,6,3,6,7,6,2,6,5,7,5,3,7,3,4,6,3,7,7,5,4,1,3,1,2,3,7,7,6,5,2,5,6,6,7,5,5,7,1,7,7,3,6,4,6,5,7,5,6,6,7,7,4,3,5,5,5,3,1,3,7,6,7,6,4,6,6,7,5,6,7,7,7,6,7,7,7,7,4,3,3,7,5,7,5,6,5,7,5,7,2,6,7,5,7,6,7,6,6,5,5,6,7,4,6,7,7,4,6,4,5,5,6,4,6,3,1,6,4,7,6,3,5,5,7,7,3,6,4,7,6,7,7,7,4,5,2,7,4,7,5,4,7,3,5,4,7,5,6,5,7,6,6,6,1,7,3,6,5,7,7,6,4,7,4,7,7,5,7,6,5,3,5,5,6,5,1,7,7,7,3,6],[4,5,2,4,5,2,3,3,5,3,4,6,5,3,5,4,4,6,4,7,4,4,7,4,4,4,5,4,4,5,3,5,4,5,5,4,5,5,4,6,4,6,6,3,5,7,1,1,2,1,3,4,3,4,5,4,5,3,5,4,3,6,4,4,6,4,3,4,4,4,3,3,2,3,4,6,7,5,4,4,3,2,2,4,6,6,4,6,4,4,4,5,4,6,6,4,3,5,3,4,6,4,3,3,4,5,3,6,6,3,3,1,3,2,3,3,7,4,4,4,4,2,4,6,6,4,3,4,2,6,4,3,6,5,4,6,6,4,4,6,6,7,5,4,4,5,3,1,4,2,5,5,7,5,4,5,6,7,5,4,6,4,7,6,3,5,5,5,6,3,3,6,5,1,5,3,3,2,4,7,2,3,7,4,6,6,7,4,5,5,4,4,4,5,6,7,6,4,5,4,3,4,4,4,5,2,4,7,4,4,6,3,4,4,5,6,1,4,3,5,4,5,5,4,2,4,3,5,4,6,4,4,4,4,6,3,4,5,4,4,7,6,6,3,4,2,1,4,5,5,6,4,4,4,3,3,5,5,1,4,4,4,5,7,5,5,4,6,5,6,3,5],[4,5,3,4,5,4,3,2,3,3,1,6,4,1,4,4,4,6,4,7,4,3,4,4,4,3,4,4,4,4,3,5,2,4,4,4,5,4,4,6,4,4,6,4,4,7,1,3,2,1,2,4,3,5,4,4,3,4,4,4,3,3,4,5,6,4,3,4,4,4,3,3,2,3,4,4,5,6,4,3,5,5,4,4,5,5,3,6,3,4,4,4,4,6,4,7,3,4,3,4,3,3,4,3,4,5,3,6,6,3,3,1,3,2,3,3,5,4,2,4,4,4,4,4,3,3,3,3,4,7,3,3,6,4,5,6,5,5,2,6,5,5,5,5,3,5,4,1,4,3,6,4,7,4,3,4,6,6,4,4,6,5,6,5,3,7,4,5,3,3,3,5,5,7,5,4,4,4,4,7,3,3,3,6,6,6,6,2,5,5,4,4,3,6,3,3,6,4,5,3,4,3,4,4,5,3,2,4,4,4,5,4,4,5,5,4,1,5,5,4,5,7,5,4,3,3,4,4,3,6,3,4,7,4,6,3,1,4,5,3,7,6,6,3,4,5,1,4,4,5,7,4,3,6,4,4,5,3,4,4,3,4,5,4,4,4,4,5,3,1,3,6],[5,5,3,4,6,4,4,3,5,3,1,6,4,1,4,4,4,6,4,7,4,3,7,4,4,4,6,4,4,4,3,5,4,4,4,4,4,4,4,6,4,5,6,6,5,7,1,4,2,2,2,4,3,6,4,3,5,5,5,4,3,5,4,6,6,4,3,4,6,4,5,3,1,3,6,6,5,6,4,3,2,4,4,4,5,6,2,6,2,2,4,4,4,6,4,6,2,5,4,2,5,4,4,3,5,5,4,6,7,5,4,1,3,1,3,3,5,4,3,4,4,4,6,3,4,4,3,2,4,7,5,3,6,4,5,7,4,4,3,6,5,5,5,5,5,4,6,3,1,4,6,6,7,4,3,4,6,7,4,4,5,5,6,5,4,4,6,5,5,4,3,4,5,4,5,4,6,4,4,7,2,4,7,5,6,5,5,3,5,5,4,4,3,7,3,7,6,4,4,4,4,3,5,4,5,4,3,5,4,4,5,5,4,5,4,5,3,5,3,4,4,5,5,4,2,3,3,5,5,6,5,4,7,4,6,3,2,4,6,4,7,6,6,5,5,5,6,4,4,5,7,5,7,7,4,4,5,5,6,4,5,4,5,6,3,4,4,6,6,5,3,6],[4,4,3,5,3,3,3,3,6,3,4,3,5,5,5,1,1,6,4,5,5,5,7,4,4,5,5,4,4,5,3,3,5,5,5,4,6,6,5,6,4,3,5,5,3,7,4,4,2,6,5,4,1,5,3,4,4,3,5,5,4,2,2,5,5,5,3,4,6,7,5,2,3,3,2,5,6,7,3,3,6,3,2,5,6,5,3,6,2,2,2,5,6,6,5,4,3,3,3,3,5,3,3,7,5,6,4,6,4,4,3,1,2,1,4,3,4,3,5,4,4,2,3,2,3,4,3,3,2,7,3,3,6,6,5,6,4,3,2,5,5,6,5,7,5,5,6,6,6,5,5,5,7,6,3,4,5,5,4,4,7,5,4,6,4,7,7,6,4,5,1,4,5,3,4,5,3,2,4,4,3,5,5,5,5,6,6,2,6,5,4,3,3,7,4,2,6,4,3,4,4,4,3,5,6,5,4,5,2,4,3,6,1,4,3,6,4,4,5,4,5,4,4,5,1,5,4,6,4,5,4,3,4,3,5,3,4,4,6,4,2,5,6,3,2,6,3,7,4,5,7,5,2,4,1,6,6,6,4,4,5,1,4,7,2,6,4,5,5,5,2,6],[4,3,3,6,6,3,3,3,7,6,1,7,6,1,4,7,1,7,5,6,6,2,7,2,5,4,7,4,4,6,7,4,2,7,3,4,7,5,5,7,1,4,4,2,6,7,1,3,2,2,5,5,2,5,3,4,5,2,6,5,5,7,6,6,5,6,3,7,7,7,5,3,6,6,5,7,6,6,6,3,6,7,5,6,6,7,7,6,3,4,7,7,6,7,6,7,2,5,4,4,6,4,3,7,6,5,3,6,1,5,5,3,2,3,4,3,4,2,4,4,4,2,6,7,6,3,3,3,2,7,3,3,6,6,1,2,7,3,5,7,4,7,5,7,7,3,5,5,4,5,7,6,7,6,3,5,1,6,7,7,6,3,7,6,3,7,7,6,7,6,3,5,5,7,5,6,5,7,5,6,2,6,5,7,6,5,5,5,6,7,5,4,3,7,7,3,7,4,5,5,4,4,4,7,7,5,6,6,6,6,5,6,7,4,5,2,6,4,6,7,6,7,6,7,1,5,2,6,6,7,6,6,7,5,7,4,1,4,3,7,1,1,6,4,6,6,1,7,5,5,7,7,7,4,1,7,5,6,7,5,5,5,5,7,4,7,4,6,6,7,2,6],[3,3,3,5,4,4,4,2,7,3,1,4,4,1,4,3,3,6,4,6,3,5,7,3,4,3,6,5,4,3,1,2,3,3,4,4,5,3,2,6,1,5,5,2,6,7,1,5,2,2,5,3,2,1,3,4,2,2,5,5,5,2,4,4,5,4,2,4,7,5,4,2,6,3,4,4,6,6,3,3,6,7,5,6,6,7,4,6,3,3,4,4,6,6,4,5,2,4,3,3,4,4,3,7,4,4,3,6,5,2,5,1,4,1,4,3,4,5,2,4,4,3,4,2,3,2,3,2,2,7,2,2,2,5,2,7,6,3,5,3,5,4,2,4,5,5,4,5,1,1,5,5,7,6,1,5,3,5,4,4,5,4,4,6,5,7,5,3,1,3,2,3,5,1,3,4,3,2,2,6,3,2,1,3,5,3,5,5,5,5,5,5,4,1,2,1,5,4,2,4,4,3,4,5,4,3,1,6,5,3,5,5,4,6,2,1,1,4,7,6,2,4,3,5,1,2,2,4,5,7,4,3,1,6,6,3,2,6,6,3,5,3,6,2,1,6,1,4,4,3,6,5,2,4,1,3,5,2,1,2,4,3,5,4,2,4,6,5,6,5,4,6],[3,4,3,5,5,5,5,3,7,4,1,4,3,2,5,7,1,6,4,6,6,5,7,3,3,3,7,5,4,4,6,5,4,5,7,4,7,5,5,7,1,2,5,3,4,7,1,4,3,2,5,3,3,3,2,4,4,4,6,5,5,5,4,4,5,5,3,5,6,5,3,2,6,5,6,5,7,5,2,3,6,7,4,7,6,7,3,6,1,2,4,5,3,7,5,6,3,4,4,3,6,5,3,7,5,4,3,6,6,4,3,2,3,3,4,3,4,5,4,3,4,3,4,7,3,2,3,4,2,7,6,4,6,5,4,4,6,3,6,6,5,4,5,4,5,6,4,5,4,4,6,6,7,6,3,5,4,6,5,5,5,5,5,7,5,7,7,5,6,5,2,5,5,1,5,5,4,7,4,6,3,4,7,5,6,5,6,6,6,5,5,4,2,4,6,6,5,4,5,5,4,2,5,5,7,4,3,7,6,5,5,6,3,4,5,3,3,6,6,7,5,6,3,6,3,5,4,5,5,6,5,5,7,6,6,4,3,6,5,5,5,2,6,3,3,4,1,7,4,5,6,6,3,4,2,4,5,6,7,4,3,4,4,7,2,7,4,6,6,6,3,5],[5,3,4,5,4,7,6,3,6,6,3,7,5,2,6,7,2,6,5,3,6,6,1,4,2,4,7,6,4,4,4,6,7,6,7,4,7,5,4,6,4,2,6,3,4,7,2,5,3,5,6,3,4,5,7,4,6,5,5,5,5,7,4,5,5,5,3,7,7,6,4,2,6,5,7,5,5,6,5,3,6,7,4,7,6,5,7,6,5,3,4,6,7,7,4,6,5,6,4,4,6,3,3,7,6,5,2,6,6,5,4,3,3,1,4,3,4,5,7,4,4,4,6,2,7,4,3,5,1,7,6,4,6,7,4,7,7,3,3,6,4,7,3,5,6,5,1,6,2,6,6,7,7,6,2,6,4,7,5,7,6,6,7,7,4,7,7,5,4,7,4,6,5,5,4,5,3,5,5,6,2,6,3,5,4,4,6,3,7,7,5,6,5,7,5,4,5,4,2,4,4,4,4,5,6,6,3,6,4,6,6,5,4,5,6,7,1,3,4,7,3,3,5,7,2,6,4,6,6,7,5,6,7,7,7,4,6,7,5,5,7,5,6,3,7,4,1,7,4,5,7,7,5,5,3,3,5,7,7,6,2,5,4,7,2,7,6,7,6,7,3,7],[4,4,4,3,6,2,5,3,2,2,2,6,2,4,6,7,3,7,5,3,2,3,3,4,1,4,5,5,3,1,3,6,3,6,6,1,4,5,5,5,3,2,6,2,1,7,1,2,3,6,6,4,4,5,3,3,6,4,1,6,2,3,5,7,5,5,2,7,6,6,3,3,7,3,6,2,6,6,5,4,1,3,2,5,5,5,7,6,1,3,3,3,5,7,6,5,4,3,4,4,6,3,3,6,6,5,4,6,5,6,3,1,2,6,4,3,2,5,6,4,3,2,4,1,4,2,4,4,1,6,4,4,6,5,1,4,5,5,5,5,5,6,5,6,6,5,3,3,1,3,5,7,7,5,3,5,3,6,5,4,6,5,4,7,5,7,6,4,3,5,3,6,5,4,7,5,3,6,4,5,3,5,4,6,5,5,6,5,6,4,4,5,4,6,6,3,6,5,4,4,4,3,3,7,6,6,1,2,4,3,5,3,3,4,1,5,1,5,4,4,4,5,4,6,4,5,2,5,4,5,5,3,7,5,5,4,3,5,4,2,3,2,6,4,2,5,2,7,4,7,6,4,1,3,4,5,4,5,4,6,4,3,4,7,1,7,5,6,4,6,2,6],[4,4,3,5,3,5,3,3,4,4,1,7,5,3,6,6,1,7,5,7,5,1,7,3,2,5,6,5,4,3,5,6,5,5,7,1,6,6,4,6,4,5,5,1,3,7,5,3,3,6,6,5,3,4,6,4,3,5,3,6,5,4,4,6,5,4,3,7,7,4,3,3,6,5,5,6,6,5,5,3,2,5,5,5,5,6,5,6,1,2,1,6,6,7,6,4,3,5,4,4,6,4,3,6,6,4,3,6,5,6,3,3,2,5,4,3,6,6,4,4,3,2,5,6,5,4,4,4,2,6,5,4,6,5,4,6,5,5,6,6,6,4,5,6,6,5,4,4,4,3,6,7,7,4,2,5,5,6,5,5,5,5,3,5,3,7,6,5,5,4,3,4,5,7,5,5,4,7,5,7,2,5,3,6,6,3,5,6,6,7,4,5,4,4,6,3,6,4,3,4,4,4,5,6,6,6,3,5,3,6,5,6,4,5,4,6,2,5,4,6,5,7,5,4,4,4,2,6,4,6,4,6,7,4,5,4,4,6,2,4,6,6,6,3,3,6,1,7,4,4,6,4,6,5,3,4,4,5,5,5,4,4,4,7,3,7,6,7,5,6,2,6],[3,4,3,4,3,3,5,3,4,3,1,7,1,3,5,6,1,6,4,7,5,2,3,2,1,4,6,6,4,2,6,6,5,6,7,1,1,2,4,5,3,4,3,1,4,7,3,4,3,3,3,3,4,2,5,3,5,2,2,1,2,2,4,5,5,1,3,7,5,6,1,3,7,4,2,5,6,5,2,5,3,1,2,5,5,7,2,6,1,2,1,5,4,6,5,7,5,1,3,3,1,3,3,6,4,4,2,6,6,4,3,2,2,2,4,3,6,5,2,3,3,4,4,2,5,3,4,3,4,7,6,3,6,6,3,6,6,4,2,6,4,6,3,6,6,4,3,4,6,5,5,5,7,4,4,5,2,6,4,2,6,5,2,7,4,4,6,6,6,5,3,2,5,5,4,5,4,6,3,7,3,4,3,7,4,4,5,2,4,5,4,6,4,3,4,2,6,4,7,3,1,3,4,5,5,4,2,2,3,1,5,6,2,4,4,1,3,2,4,2,4,7,5,4,5,3,2,6,5,6,5,4,7,6,4,3,6,7,1,4,3,6,3,4,5,5,1,7,4,7,6,5,6,5,4,5,4,4,7,2,4,4,4,5,4,7,4,6,5,5,2,6],[3,4,2,3,4,3,5,3,3,4,2,5,4,4,4,5,1,6,4,6,4,2,3,4,1,4,6,4,4,3,2,5,2,4,3,1,4,4,4,5,4,5,4,4,3,6,1,1,3,5,3,4,2,1,5,3,2,2,4,2,3,2,4,6,5,3,3,7,5,5,3,3,6,3,4,4,6,6,4,4,4,3,4,4,6,6,4,4,1,3,4,5,2,6,5,6,4,4,4,3,5,5,3,6,4,4,2,6,6,5,4,1,2,3,4,3,4,4,4,5,3,4,3,4,3,3,4,4,1,6,4,4,6,4,4,6,4,4,2,6,6,5,4,6,4,4,3,2,2,1,5,6,7,4,3,3,2,6,5,4,5,4,4,6,2,4,4,4,2,5,3,5,5,7,5,4,5,3,4,6,3,3,7,6,5,4,5,2,4,5,4,5,4,4,4,5,6,5,5,6,4,3,4,5,4,3,1,5,2,4,5,2,4,4,3,4,3,4,4,4,4,7,5,4,3,5,1,5,5,5,4,4,4,6,5,4,3,4,4,3,3,4,6,4,4,4,4,4,4,5,6,4,1,5,4,3,4,4,4,2,3,3,4,4,1,4,4,6,5,5,2,6],[2,4,3,3,5,5,5,4,5,3,2,5,5,2,5,4,1,7,3,6,6,2,3,4,5,3,4,5,4,4,3,5,6,6,5,1,6,6,4,6,4,5,5,4,4,6,4,3,3,4,2,4,2,5,5,3,3,4,5,2,6,4,1,6,5,4,3,7,7,5,5,5,6,3,5,2,6,5,4,4,5,4,3,5,6,6,4,5,2,3,3,4,7,6,5,4,2,4,4,7,6,4,3,6,5,3,4,6,5,5,4,1,2,5,4,3,4,2,5,6,3,5,2,1,4,2,3,3,1,7,6,1,6,6,3,6,6,3,5,6,4,6,2,6,2,5,3,4,4,2,5,5,7,4,4,6,3,6,5,5,6,6,4,7,4,7,4,3,6,4,3,5,5,6,7,4,4,3,3,6,2,6,7,6,2,6,4,5,4,3,4,3,3,4,6,7,6,5,4,4,4,4,4,4,5,4,2,3,4,2,5,5,4,4,6,4,3,3,4,4,6,7,6,6,4,5,1,6,4,6,5,5,7,6,6,4,1,5,5,5,5,4,6,3,4,7,4,4,4,5,6,5,3,5,4,4,4,5,7,3,3,3,4,3,3,2,4,6,5,4,2,6],[4,3,4,5,4,3,4,3,7,4,4,4,5,1,5,4,1,6,4,6,4,3,7,3,4,4,5,4,4,4,4,4,4,4,5,1,4,4,4,5,5,3,4,2,3,6,4,2,4,4,4,4,3,4,5,4,3,4,4,4,5,4,4,6,2,5,3,3,7,4,4,4,5,5,5,5,5,6,5,1,3,5,4,6,6,6,6,6,1,4,3,4,7,6,4,6,2,4,5,7,7,4,3,6,4,5,4,6,5,4,3,1,3,1,4,3,5,5,6,5,3,4,4,6,5,3,3,4,5,6,5,3,4,5,5,5,6,4,6,6,7,6,4,6,5,5,3,5,5,3,5,6,3,4,4,5,4,6,5,5,7,4,5,6,4,4,7,5,4,6,3,4,5,6,5,7,4,6,4,7,3,6,7,6,5,5,5,6,7,5,4,4,3,5,6,7,4,6,3,5,2,4,3,3,5,4,5,2,5,4,4,6,3,5,7,6,4,4,5,5,4,5,5,4,2,5,2,4,3,6,5,5,7,2,5,4,4,3,6,7,6,6,6,4,3,4,4,4,4,6,6,4,2,4,3,4,4,5,6,2,4,3,5,4,3,5,4,6,5,4,3,5],[4,3,3,5,5,3,3,3,7,4,4,4,5,1,5,4,1,6,4,6,4,3,7,3,4,4,5,4,4,4,4,3,4,4,5,1,4,5,4,5,5,2,4,2,4,6,4,1,4,4,4,5,3,4,5,4,4,4,4,4,4,4,4,6,4,5,3,3,7,4,4,4,5,4,5,5,5,5,5,3,3,6,4,6,5,6,5,6,2,3,3,4,7,6,4,6,1,4,5,7,6,3,3,6,4,5,4,6,5,4,4,1,3,2,4,3,5,5,6,5,3,4,4,6,5,3,3,3,5,6,4,5,4,5,5,5,6,4,6,6,7,6,4,5,5,5,3,4,5,3,5,5,4,4,3,5,3,6,5,5,7,4,6,6,4,4,7,4,5,4,3,4,5,4,6,6,4,5,4,6,3,6,7,4,5,6,5,6,5,5,4,4,4,5,6,7,4,6,3,5,2,3,3,6,5,6,5,3,5,4,4,6,3,4,7,6,4,4,5,5,4,5,5,4,1,6,2,4,3,6,4,5,7,6,5,4,5,3,6,6,3,6,6,3,3,4,3,4,4,6,7,4,2,4,3,6,4,5,6,2,4,3,5,4,3,3,4,6,5,4,3,5],[4,3,3,4,4,3,4,3,7,4,4,5,5,1,5,3,1,6,4,5,4,3,7,3,4,4,5,4,4,4,4,3,3,4,5,1,4,5,4,5,7,2,4,2,2,6,4,1,4,4,5,5,3,4,4,4,6,5,5,4,5,5,4,6,4,4,3,3,7,4,4,5,5,6,6,5,5,5,4,2,3,6,4,6,5,6,5,6,1,3,3,4,7,6,4,6,1,4,5,7,6,4,3,6,4,5,2,6,5,3,4,1,3,1,4,3,5,5,7,5,3,5,4,6,5,2,4,4,5,4,4,3,4,5,5,5,6,4,6,6,7,6,4,6,6,6,4,6,4,3,5,6,6,4,3,6,3,6,5,5,7,4,6,2,4,4,7,4,5,7,3,4,5,4,6,7,5,5,2,7,3,6,7,5,6,5,5,6,4,5,5,3,4,5,6,7,4,6,3,4,2,3,4,6,6,5,5,2,5,4,4,6,4,4,5,6,4,4,5,4,5,6,3,4,1,6,2,4,3,6,4,5,7,6,6,4,6,3,6,7,5,6,6,4,2,4,2,4,4,6,6,4,1,4,3,5,4,5,2,2,4,3,5,4,3,4,4,6,5,4,3,5],[4,3,3,3,3,5,3,3,4,3,3,5,4,1,5,1,3,6,5,3,3,3,4,3,1,2,5,6,2,4,3,4,5,6,6,1,5,6,3,5,3,2,5,1,1,7,4,3,4,1,5,4,2,3,2,4,1,2,4,2,3,4,5,5,1,4,3,4,3,3,2,4,2,5,6,5,6,5,5,3,5,3,3,5,5,6,5,6,5,2,2,5,6,5,5,7,1,5,3,4,5,3,3,5,4,7,1,6,5,7,4,1,3,1,4,3,4,5,3,3,3,4,5,4,4,4,4,4,1,7,3,2,2,4,4,5,3,4,6,4,6,5,4,5,5,5,2,2,5,2,5,4,3,6,4,3,2,2,1,5,4,4,7,5,4,1,7,5,3,2,3,6,5,1,5,5,4,5,3,5,2,5,7,2,6,5,5,6,6,5,4,3,3,5,4,6,3,6,6,4,2,3,3,5,5,3,2,3,1,4,4,7,7,5,4,7,4,3,4,5,5,7,4,5,1,2,4,5,3,7,4,4,4,5,5,4,1,4,6,6,5,6,6,1,4,5,3,5,3,5,7,4,1,3,4,3,7,5,2,3,3,5,4,6,2,5,2,7,4,4,3,5],[4,3,3,1,2,5,3,3,4,3,3,5,3,1,5,1,3,7,5,3,3,1,4,2,1,2,5,6,4,5,3,4,5,7,4,1,5,6,4,5,3,2,5,1,1,7,4,3,4,1,2,4,2,3,3,3,4,2,4,1,2,4,4,4,1,4,3,3,2,3,2,3,3,6,6,6,6,6,5,2,4,2,2,4,4,6,4,6,3,3,3,5,6,5,5,6,2,4,3,6,5,3,3,5,3,7,2,6,5,7,3,1,2,1,4,3,4,6,2,4,3,3,5,2,4,3,3,3,1,6,4,2,2,3,1,5,4,5,5,4,4,4,5,5,6,5,1,2,3,2,4,3,4,6,2,3,2,1,1,5,4,4,7,5,4,1,5,5,3,2,3,5,5,1,7,3,4,5,3,4,1,5,7,1,6,6,5,5,5,5,4,3,4,5,4,7,3,5,5,4,2,3,3,5,4,4,1,3,1,4,4,7,7,4,3,7,4,4,4,5,5,7,5,6,1,2,3,5,3,7,4,4,5,6,4,4,1,4,6,6,5,6,6,1,4,5,1,5,3,5,5,5,2,3,4,2,7,4,2,1,3,5,3,6,2,6,3,6,4,5,2,5],[4,1,3,3,1,7,3,3,6,3,3,5,5,1,6,1,3,6,5,2,2,2,4,4,1,4,7,6,4,4,1,6,6,7,4,1,5,7,2,6,3,1,3,1,1,7,4,1,4,2,3,4,2,3,4,3,3,2,3,1,3,3,6,5,1,4,3,7,2,3,3,5,3,6,7,2,5,4,2,3,3,1,1,5,5,6,5,6,2,3,1,3,7,5,5,5,3,4,5,5,4,3,3,5,4,7,1,6,5,7,3,1,2,1,4,3,2,6,1,4,3,3,4,5,3,2,3,3,1,7,1,2,2,6,2,5,5,6,1,2,6,5,2,6,6,6,4,5,3,4,1,6,4,6,1,5,2,2,1,7,3,5,7,4,4,3,5,7,7,2,3,6,5,1,7,2,4,4,3,7,3,5,4,3,6,6,4,1,5,4,3,3,4,5,7,7,2,5,4,2,2,5,4,5,7,5,2,6,1,5,4,7,7,4,2,7,4,2,5,3,5,5,2,5,1,3,7,5,3,6,5,5,7,6,4,6,7,5,7,6,5,5,4,1,4,6,1,1,4,2,6,5,4,3,4,5,4,6,7,2,4,5,4,7,4,6,2,5,2,6,2,6],[3,1,3,4,2,3,2,2,3,3,1,6,5,1,3,2,3,6,4,5,2,1,7,1,1,3,5,4,4,4,1,5,2,4,4,1,4,5,4,6,4,1,4,1,1,5,4,1,4,1,3,3,4,1,4,3,4,4,1,4,3,1,3,5,3,4,3,4,3,4,2,3,2,3,5,4,5,4,4,2,3,3,4,4,5,5,6,7,1,3,1,2,4,6,4,5,1,2,4,7,6,3,3,5,3,4,5,6,5,4,4,1,2,1,4,3,3,5,4,4,3,4,1,2,2,2,3,4,3,6,1,3,2,5,3,4,6,5,3,6,4,4,5,6,5,5,3,1,5,4,4,4,3,6,5,3,4,4,2,4,6,4,4,6,4,1,7,5,4,6,3,2,4,5,7,5,4,5,2,6,2,4,7,3,6,5,4,3,5,5,4,2,2,5,6,7,4,5,2,2,2,3,4,6,4,3,2,2,3,4,4,6,6,4,2,6,4,3,4,5,5,7,3,5,1,4,3,4,3,5,4,5,4,4,4,5,6,4,6,6,4,5,5,3,1,2,2,4,3,6,6,4,1,3,4,3,1,5,2,1,1,3,5,7,1,5,4,5,3,4,2,5],[3,2,3,4,2,3,4,3,5,3,1,6,5,1,5,2,3,6,4,5,4,1,7,4,4,3,5,6,5,4,1,5,5,4,5,1,4,6,4,4,5,1,4,1,1,5,4,1,4,3,3,3,2,4,4,3,5,3,2,4,4,2,3,5,3,5,3,5,4,4,4,5,3,7,7,4,5,3,4,2,4,3,3,5,5,6,6,6,1,3,1,2,3,6,4,5,1,2,4,7,6,3,3,5,3,4,6,6,5,4,4,1,2,1,4,3,3,5,5,4,3,3,1,6,2,3,3,4,2,7,3,3,3,3,4,6,5,5,5,6,7,5,5,5,6,4,4,5,3,4,4,4,3,5,3,5,4,5,2,7,5,5,4,6,3,1,7,5,7,4,3,2,4,5,7,5,6,5,4,7,2,4,7,4,6,5,6,5,6,5,4,3,2,5,4,7,4,5,4,4,2,4,4,5,6,3,4,4,5,4,5,7,7,5,4,7,2,3,4,5,5,7,5,3,1,4,3,4,3,6,4,5,7,6,5,5,6,5,5,2,4,4,6,3,3,5,3,4,3,6,6,5,5,3,4,5,5,3,6,1,2,3,4,5,4,5,4,5,3,6,2,5],[3,2,2,5,1,3,2,2,2,3,1,2,5,1,4,2,3,7,4,5,4,1,1,2,1,3,6,6,5,3,1,3,3,4,4,1,4,3,4,4,4,1,4,1,1,5,1,1,4,3,3,4,4,2,4,3,5,3,1,4,3,2,4,5,1,5,3,6,1,4,3,5,2,4,6,4,4,5,4,3,5,3,2,4,6,6,6,6,1,3,1,2,3,3,4,7,1,2,4,7,6,4,3,5,3,4,2,6,5,5,4,1,2,1,4,3,3,4,5,4,3,2,1,3,2,1,1,3,7,6,1,3,3,3,4,1,4,5,1,6,7,5,5,6,6,1,4,1,4,3,2,4,3,6,4,4,4,4,2,7,5,4,1,6,4,1,7,5,7,5,3,2,4,5,7,3,5,5,2,7,2,4,7,3,6,5,4,1,6,5,5,1,2,4,7,7,4,5,4,2,3,3,4,5,5,3,4,4,3,4,5,6,7,5,2,6,2,5,4,5,5,1,4,3,1,4,3,4,1,6,3,5,3,6,5,5,6,5,4,2,2,2,5,4,2,1,1,4,3,6,4,4,7,3,4,4,1,4,2,2,2,3,4,6,2,5,4,3,3,4,2,4],[4,5,3,5,2,2,6,7,6,5,6,6,6,7,7,3,4,6,7,3,6,3,7,5,5,5,6,5,6,5,5,5,5,4,5,6,5,7,4,6,5,2,6,6,7,7,5,6,3,6,6,6,4,5,7,5,6,6,6,5,4,6,5,6,5,4,3,6,5,5,5,6,5,5,7,5,6,3,6,3,5,7,5,4,5,5,7,6,6,4,4,6,7,5,7,7,4,6,6,7,5,3,3,5,5,7,5,6,5,5,4,2,2,2,4,3,7,4,7,5,3,6,5,4,4,5,4,4,7,6,6,3,7,5,3,6,7,5,7,7,7,6,5,7,6,5,3,6,3,6,7,6,4,6,4,3,4,5,7,4,4,6,5,6,4,4,7,6,7,6,3,5,4,7,5,5,5,7,5,7,3,5,7,3,7,5,4,7,4,7,4,5,4,7,7,7,7,6,4,5,2,4,4,6,6,6,4,5,6,4,6,7,4,5,7,7,4,7,5,4,5,7,6,5,7,3,5,6,5,7,5,4,7,7,6,5,7,5,5,4,4,6,6,4,5,4,7,7,3,5,6,6,3,3,4,7,7,7,2,4,5,4,4,6,4,7,5,6,1,6,2,6],[4,5,2,5,5,3,5,7,6,5,3,6,6,6,7,3,3,6,7,4,6,3,7,5,4,4,6,5,6,4,5,5,5,4,5,5,5,7,3,6,5,4,6,6,7,7,5,6,3,6,6,6,2,6,7,3,4,6,6,5,3,6,4,6,5,4,3,6,5,5,5,6,4,5,7,5,6,4,6,3,5,7,5,4,6,5,7,6,6,4,4,5,7,4,7,7,3,6,6,7,5,3,3,5,5,7,5,6,5,5,4,2,2,2,4,3,7,4,7,5,3,6,5,2,4,5,4,3,7,6,6,3,7,5,4,6,7,5,6,7,7,6,5,7,6,5,3,6,3,5,7,5,3,6,4,5,5,5,7,4,5,5,5,6,5,4,7,6,7,6,3,5,4,7,7,5,5,7,4,7,3,5,7,5,7,5,5,6,4,7,4,4,4,7,6,7,6,6,3,3,3,4,4,6,6,5,4,5,6,4,6,7,4,5,7,7,3,7,5,4,5,7,5,5,6,4,5,6,5,7,5,4,7,7,6,4,7,5,5,4,4,6,6,4,4,4,7,7,3,5,6,4,3,3,4,7,7,7,4,4,5,4,5,6,4,7,5,6,2,6,2,6],[4,5,2,6,4,3,5,7,6,5,2,5,7,6,6,3,4,6,6,4,4,3,7,5,4,5,5,4,6,2,5,5,5,4,5,5,5,7,4,6,5,3,6,6,7,7,5,6,3,6,7,6,4,6,7,4,5,6,6,5,4,6,4,6,5,4,3,6,7,5,5,6,4,4,7,6,6,3,5,3,6,7,4,4,5,4,5,6,4,4,4,6,7,5,7,7,1,6,6,7,5,3,3,5,5,7,3,6,5,3,4,6,3,2,4,3,7,4,7,5,3,6,5,6,4,5,4,4,7,6,6,3,7,6,4,6,7,5,7,7,7,6,6,7,6,5,3,5,7,2,7,4,4,6,4,4,5,5,7,4,5,5,5,6,5,4,7,6,5,6,3,5,4,7,6,5,5,5,5,7,3,6,7,4,7,5,5,7,6,6,4,4,4,7,6,7,6,6,3,4,2,4,4,6,5,4,7,5,6,4,6,7,6,5,7,7,4,7,5,4,5,7,3,5,3,4,4,6,5,7,6,5,7,7,6,4,7,5,6,4,4,5,6,4,6,4,2,7,3,5,5,4,1,3,4,7,7,5,7,4,4,4,4,7,5,4,5,7,5,6,3,6],[4,5,2,6,1,3,6,5,6,5,4,5,7,1,7,6,5,6,6,4,5,3,7,4,4,4,6,5,6,3,5,5,6,4,7,5,5,5,4,6,5,5,6,3,5,7,5,6,3,6,7,6,3,5,7,4,5,6,6,5,3,6,4,6,3,4,3,6,6,5,5,6,4,7,7,6,6,5,7,3,4,6,3,4,5,4,6,6,4,4,4,5,7,4,7,6,2,6,5,7,5,4,3,5,5,7,4,6,5,3,4,3,2,1,4,3,7,4,7,5,3,6,5,3,4,6,4,3,7,7,4,3,5,6,3,6,7,5,2,7,7,6,7,7,6,5,3,5,4,4,7,4,3,6,3,4,5,5,7,4,5,4,5,6,5,4,7,6,7,6,3,5,4,7,6,5,5,7,5,7,3,5,7,5,7,5,5,2,4,7,4,4,4,7,6,7,6,5,3,3,2,4,4,6,5,6,5,5,4,4,6,5,3,5,7,7,4,7,5,4,5,7,5,5,7,4,4,6,2,7,5,5,5,7,6,5,7,5,6,3,4,6,6,4,3,4,7,7,4,5,5,4,6,3,4,7,7,7,7,4,5,4,5,7,5,4,5,6,2,6,2,6],[2,3,2,3,2,1,2,2,3,2,3,3,3,2,3,1,2,3,3,3,2,2,2,3,2,3,2,2,3,2,3,3,3,2,1,2,2,3,2,3,3,2,2,2,3,3,3,2,3,2,3,3,2,2,3,3,3,3,3,2,2,3,1,3,2,3,3,3,3,3,3,3,2,1,3,3,2,1,3,3,3,3,2,2,3,2,3,3,3,3,2,2,3,3,2,2,1,2,3,3,3,3,2,3,2,3,2,3,2,2,2,1,2,1,2,2,3,2,3,3,3,3,3,3,2,2,3,1,3,3,3,3,2,3,2,3,3,3,3,3,3,2,1,3,2,3,2,3,3,2,2,2,2,3,2,2,3,2,3,3,3,3,3,3,3,2,3,3,2,3,2,3,2,3,2,3,3,3,2,3,3,3,3,3,3,3,2,3,2,3,3,2,3,3,3,3,3,2,2,3,2,2,2,3,2,3,3,2,2,3,3,2,2,2,3,3,2,3,2,2,3,3,2,2,1,2,3,2,3,3,3,3,3,3,3,3,3,3,3,2,3,3,3,3,2,3,3,3,2,3,2,2,2,3,3,3,3,3,3,2,2,3,2,3,2,2,2,3,3,3,2,2],[1,4,4,3,5,3,3,5,5,4,5,4,5,5,5,6,4,6,4,5,5,6,4,3,7,4,4,3,6,6,6,6,5,5,5,7,5,5,6,5,5,5,6,5,4,6,6,4,4,6,6,5,5,4,6,4,6,5,6,7,4,2,6,6,5,4,4,4,7,5,6,6,5,5,5,5,4,6,4,4,4,5,5,5,6,5,5,6,5,3,4,5,6,6,5,4,4,4,1,5,7,7,3,4,3,5,5,6,5,5,5,6,2,4,4,3,5,3,5,6,5,2,6,6,5,4,4,3,7,6,4,4,6,4,3,6,6,5,6,5,5,5,6,6,5,5,3,2,6,4,6,5,2,5,4,4,5,4,5,4,5,2,6,6,4,6,5,5,6,5,3,5,5,1,4,5,5,7,3,6,3,5,5,6,6,5,6,6,5,5,4,2,3,6,5,4,5,5,4,6,5,3,5,5,5,6,3,6,4,6,6,4,5,4,6,5,3,6,6,5,6,7,5,5,4,5,4,6,4,6,5,2,7,4,4,4,1,5,4,6,2,3,6,3,6,6,4,5,4,4,7,5,3,3,4,5,5,5,4,5,5,4,4,4,6,5,2,5,5,4,2,7],[1,4,4,3,6,4,3,4,5,4,5,5,6,4,5,6,5,6,5,5,4,6,4,4,7,4,5,4,6,6,6,5,5,5,6,7,5,6,6,5,5,6,6,5,4,7,6,5,5,5,6,5,4,4,7,4,5,5,6,5,6,3,6,6,5,4,4,6,7,6,6,7,5,5,5,5,5,6,6,4,5,5,5,5,6,5,5,6,6,3,4,5,6,7,5,3,5,4,3,5,6,7,3,4,3,5,6,6,5,5,4,6,3,5,4,3,5,3,5,7,4,2,6,5,5,5,3,3,7,6,4,4,7,5,5,7,6,5,6,7,7,5,5,6,5,7,3,2,7,4,6,6,3,5,4,5,5,7,5,4,7,2,6,5,4,7,5,5,6,5,3,5,5,1,4,5,6,7,3,6,3,6,5,6,6,5,6,6,6,6,4,2,3,6,6,6,5,5,4,6,5,4,5,5,5,6,3,5,5,6,4,4,5,4,7,5,5,6,5,5,6,3,5,6,4,6,6,6,4,6,5,2,7,5,2,4,2,4,5,6,3,4,6,3,7,7,4,7,4,4,7,5,5,3,4,6,5,5,5,5,5,4,5,5,6,5,4,5,6,3,3,7],[2,4,4,4,6,5,5,4,5,4,5,7,6,7,5,6,4,6,5,6,6,6,4,3,7,3,4,5,6,6,7,5,6,7,7,7,5,6,6,5,5,6,6,5,4,7,6,7,5,4,7,5,6,4,7,4,6,5,6,6,5,3,6,6,5,6,4,6,7,6,6,7,6,5,6,6,6,7,6,4,4,5,5,5,5,6,5,6,6,3,4,6,6,7,7,4,6,4,3,5,7,7,3,4,3,5,7,7,5,5,5,6,2,2,4,3,6,5,5,7,4,2,6,6,5,5,3,3,7,6,5,4,7,6,7,7,7,5,7,7,7,5,6,6,5,7,4,4,7,4,6,7,4,5,6,4,7,5,6,5,7,2,6,5,4,7,5,5,7,5,3,6,5,7,4,6,6,7,3,7,3,7,6,6,7,4,7,7,6,7,5,2,4,6,6,6,5,5,5,6,5,4,5,5,5,6,5,7,4,6,5,4,5,4,7,6,5,6,5,6,6,4,5,6,4,6,6,6,5,6,5,2,7,6,3,4,5,5,6,7,4,3,6,3,7,7,4,7,4,5,7,6,5,3,4,6,6,6,5,5,6,4,4,7,6,6,2,5,5,4,2,7],[2,4,4,3,6,3,3,4,2,4,5,5,5,4,5,6,3,6,6,6,6,6,4,4,7,6,3,3,6,5,6,5,6,7,6,7,5,5,5,5,5,6,6,5,3,7,6,6,5,7,7,5,7,4,7,5,6,4,6,6,6,4,6,6,3,6,4,2,7,6,6,7,5,4,6,6,6,6,6,4,5,3,5,5,5,5,5,7,7,3,4,4,6,7,7,5,6,4,4,6,5,6,3,4,3,5,7,6,5,2,5,6,3,1,4,3,5,2,7,7,4,2,6,5,5,4,4,3,7,6,4,3,7,5,7,7,6,5,7,5,6,5,4,6,5,4,5,1,7,4,5,6,3,5,2,4,5,7,4,5,7,3,5,5,4,7,5,5,5,5,3,5,4,1,4,6,6,7,4,7,3,6,2,7,7,4,6,7,6,6,5,2,4,6,6,7,5,5,6,7,5,3,5,5,5,5,5,7,3,6,7,5,4,5,7,4,5,6,3,6,5,1,5,6,4,4,4,6,3,6,5,2,7,4,2,4,2,4,6,6,2,5,6,3,6,6,4,5,4,5,7,6,5,3,4,6,5,5,4,5,6,3,5,6,6,7,2,4,5,3,3,7],[2,4,3,2,6,4,5,7,3,4,5,4,6,1,5,6,3,6,5,6,6,6,4,3,7,4,2,4,5,6,6,5,3,4,7,7,5,7,6,5,4,6,6,1,5,7,4,5,6,7,5,5,7,2,6,5,5,6,6,6,4,4,6,5,2,3,4,5,5,5,7,6,5,4,6,7,6,7,5,4,4,3,5,4,5,5,5,6,6,3,4,6,6,6,6,4,6,4,4,6,5,7,3,4,2,5,4,7,5,6,4,2,2,1,4,3,4,2,5,7,4,1,6,5,5,3,4,3,7,6,6,4,2,3,3,7,7,4,6,6,4,5,2,6,5,5,5,2,7,4,5,6,3,5,5,2,5,1,5,5,7,2,4,4,3,7,5,5,6,4,3,2,4,1,4,2,6,7,4,7,3,7,6,6,6,5,5,6,6,6,5,6,3,6,6,5,5,5,3,5,4,5,4,4,5,4,3,7,2,4,5,7,5,5,2,3,5,6,4,3,4,1,4,6,4,4,5,6,3,7,5,2,4,6,3,4,6,3,6,5,4,3,5,3,7,6,4,4,4,5,7,5,1,3,2,6,3,5,7,5,6,7,4,3,5,5,3,5,2,5,2,7],[\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"November\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\",\"January\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>respondent<\\/th>\\n      <th>bususefrequency<\\/th>\\n      <th>age<\\/th>\\n      <th>gender<\\/th>\\n      <th>bicycleuse<\\/th>\\n      <th>bikeuse<\\/th>\\n      <th>caruse<\\/th>\\n      <th>bususe<\\/th>\\n      <th>walking<\\/th>\\n      <th>bt1<\\/th>\\n      <th>bt2<\\/th>\\n      <th>bt3<\\/th>\\n      <th>bt4<\\/th>\\n      <th>bt5<\\/th>\\n      <th>bt6<\\/th>\\n      <th>bt7<\\/th>\\n      <th>bd1<\\/th>\\n      <th>bd2<\\/th>\\n      <th>bd3<\\/th>\\n      <th>bd4<\\/th>\\n      <th>bcm1<\\/th>\\n      <th>bcm2<\\/th>\\n      <th>bcm3<\\/th>\\n      <th>bcm4<\\/th>\\n      <th>bter1<\\/th>\\n      <th>bter2<\\/th>\\n      <th>bter3<\\/th>\\n      <th>bter4<\\/th>\\n      <th>bter5<\\/th>\\n      <th>emp1<\\/th>\\n      <th>emp2<\\/th>\\n      <th>emp3<\\/th>\\n      <th>emp4<\\/th>\\n      <th>emp5<\\/th>\\n      <th>cs1<\\/th>\\n      <th>cs2<\\/th>\\n      <th>cs3<\\/th>\\n      <th>pv1<\\/th>\\n      <th>pv2<\\/th>\\n      <th>pv3<\\/th>\\n      <th>wom1<\\/th>\\n      <th>wom2<\\/th>\\n      <th>wom3<\\/th>\\n      <th>ep1<\\/th>\\n      <th>ep2<\\/th>\\n      <th>ep3<\\/th>\\n      <th>ep4<\\/th>\\n      <th>s_qcomparison<\\/th>\\n      <th>ls1<\\/th>\\n      <th>ls2<\\/th>\\n      <th>ls3<\\/th>\\n      <th>ls4<\\/th>\\n      <th>ls5<\\/th>\\n      <th>weather<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\n\n## Exploratory factor analysis\n\n### Scree plot\n\nWe can use a scree test to identify the optimum number of factors that can be extracted. In the example below, a parallel analysis was used to identify the potential number of factors that can be extracted.\n\nThe test will generate simulated dataset with random values for the same number of variables and sample size. The test involved the following procedures [see @hair_multivariate_2019]:\n\n- Then each of these simulated datasets is then factor analyzed, either with PCA or CFA and the eigen values are averaged for each factor across all the data sets.\n\n- The results is the average eigenvalues for the first, second and so on across the set of simulated dataset.\n\n- These values are then compared to the eigen values extracted for the original data.\n\n- All factors with eigen values above those average eigen values are retained.\n\n\nResult of the parallel analysis shows that there are 6 potential factors (i.e., six triangles are above the red broken line or the simulated data). At this stage, you compare your expected number of factors to be extracted against the scree test. Note that the test can be used as one of your bases for factor extraction. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Scree plot using parallel analysis\nfa.parallel(case_data_items, fa = \"fa\")\n```\n\n::: {.cell-output-display}\n![](06-cb-sem-sample_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nParallel analysis suggests that the number of factors =  7  and the number of components =  NA \n```\n:::\n:::\n\n\n### Factor extraction\n\nAn EFA with a varimax rotation was used to identify which items load to their expected construct. Items with high cross-loadings (i.e., items which load highly to more than one construct) and loadings below 0.30 were excluded.\n\nHigh cross-loadings pose difficulty in establishing a separate concept of each variable when factors are apparently shared variables. The goal is to identify the pattern where each item associates only with one factor.\n\nSome guides for factor loadings [@hair_multivariate_2019]:\n\n- Factor loadings less than ±0.10 can be considered equivalent to zero for purposes of assessing simple structure.\n\n- Factor loadings in the range of ±0.30 and ±0.40 are considered to meet the minimal level for interpretation of the structure.\n\n- Loadings ±0.50 or greater are considered practically significant\n\n- Loadings exceeding ±0.70 are considered indicative of well-defined structure and are the goal of any factor analysis.\n\nExtremely high loadings such as ±0.90 and above are not typical, and the practical significance of the loadings is an important criterion.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Factor loading\nbus_fa <- fa(r = case_data_items,\n             nfactors = 6,\n             rotate = \"varimax\")\n\nprint(bus_fa$loadings, sort = TRUE, cutoff = 0.4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nLoadings:\n     MR2   MR1   MR3   MR4   MR6   MR5  \nls1  0.820                              \nls2  0.891                              \nls3  0.828                              \nls4  0.806                              \nls5  0.599                              \nbt1        0.673                        \nbt2        0.666                        \nbt4        0.549                        \nbt5        0.680                        \nbt6        0.578                        \nbt7        0.550                        \nep1              0.864                  \nep2              0.900                  \nep3              0.690                  \nep4              0.705                  \nemp1                   0.688            \nemp2                   0.662            \nemp3                   0.636            \nemp4                   0.697            \nemp5                   0.502            \nbd1                          0.679      \nbd2                          0.640      \nbd3                          0.676      \nbd4                          0.629      \ncs1                                0.774\ncs2                                0.817\ncs3                                0.768\nbt3        0.476                        \n\n                 MR2   MR1   MR3   MR4   MR6   MR5\nSS loadings    3.477 3.363 3.081 2.658 2.429 2.297\nProportion Var 0.124 0.120 0.110 0.095 0.087 0.082\nCumulative Var 0.124 0.244 0.354 0.449 0.536 0.618\n```\n:::\n:::\n\n\n## Confirmatory factor analysis\n\n\n### Multivariate normality test\n\nOne of the assumptions in CFA and CB-SEM is multivariate normality. Following the test employed in the sample study, the `mvn` function of the `MVN` package [@korkmaz_mvn_2014] allows us to run Mardia's test for multivariate normality.\n\nUsing the Mardia test wherein p-value < 0.5 indicate rejection of the null hypothesis of multivariate and univariate normality. Result shows that the null hypothesis of multivariate normality of all the multivariate normality was rejected. Therefore, the study use the maximum likelihood robust (MLR) estimator to estimate the measurement model instead of the usual maximum likelihood estimator (MLR) [@rosseel_lavaan].\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMVN::mvn(data = case_data_items, mvnTest = \"mardia\", desc = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$multivariateNormality\n             Test        Statistic               p value Result\n1 Mardia Skewness 7270.01984441523 4.08329952017666e-186     NO\n2 Mardia Kurtosis 31.3657300202103                     0     NO\n3             MVN             <NA>                  <NA>     NO\n\n$univariateNormality\n               Test  Variable Statistic   p value Normality\n1  Anderson-Darling    bt1       7.2262  <0.001      NO    \n2  Anderson-Darling    bt2       8.0671  <0.001      NO    \n3  Anderson-Darling    bt3       6.6800  <0.001      NO    \n4  Anderson-Darling    bt4       6.4183  <0.001      NO    \n5  Anderson-Darling    bt5       6.1728  <0.001      NO    \n6  Anderson-Darling    bt6       8.6698  <0.001      NO    \n7  Anderson-Darling    bt7       7.5770  <0.001      NO    \n8  Anderson-Darling    bd1       5.9732  <0.001      NO    \n9  Anderson-Darling    bd2       8.7052  <0.001      NO    \n10 Anderson-Darling    bd3       5.9169  <0.001      NO    \n11 Anderson-Darling    bd4       6.8989  <0.001      NO    \n12 Anderson-Darling   emp1       5.2108  <0.001      NO    \n13 Anderson-Darling   emp2       6.4303  <0.001      NO    \n14 Anderson-Darling   emp3       4.7131  <0.001      NO    \n15 Anderson-Darling   emp4       7.8380  <0.001      NO    \n16 Anderson-Darling   emp5       5.5762  <0.001      NO    \n17 Anderson-Darling    cs1       6.9341  <0.001      NO    \n18 Anderson-Darling    cs2       7.1469  <0.001      NO    \n19 Anderson-Darling    cs3       6.2791  <0.001      NO    \n20 Anderson-Darling    ep1       7.5913  <0.001      NO    \n21 Anderson-Darling    ep2       7.3530  <0.001      NO    \n22 Anderson-Darling    ep3       7.2375  <0.001      NO    \n23 Anderson-Darling    ep4       6.5793  <0.001      NO    \n24 Anderson-Darling    ls1       9.0564  <0.001      NO    \n25 Anderson-Darling    ls2       8.2974  <0.001      NO    \n26 Anderson-Darling    ls3       8.5914  <0.001      NO    \n27 Anderson-Darling    ls4       6.9281  <0.001      NO    \n28 Anderson-Darling    ls5       5.8779  <0.001      NO    \n```\n:::\n:::\n\n\n\n### Specifying the measurement model\n\nBelow is the syntax used in defining our factor. For example, the `drivers_quality` factor is specified using the syntax `drivers_quality =~ bd1 + bd2 + bd3 + bd4`, which means this factor is measured using `bd1` to `bd4`. \n\nNote that we know which indicators to use to represent the factor based on our exploratory factor analysis in the previous sections.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Specifying CFA model\ncfa_model <- \"tangible =~ bt1 + bt2 + bt4 + bt5 + bt6 + bt7\n              drivers_quality =~ bd1 + bd2 + bd3 + bd4\n              empathy =~ emp1 + emp2 + emp3 + emp4 + emp5\n              env_perf =~ ep1 + ep2 + ep3 + ep4\n              customer_sat =~ cs1 + cs2 + cs3\n              life_sat =~ ls1 + ls2 + ls3 + ls4 + ls5\"\n```\n:::\n\n\n### Fitting the model\n\nGiven the result of the normality test, a maximum likelihood robust (MLR) was used to estimate the initial measurement model. Based on the assessment criteria, the model shows an acceptable and adequate model fit.\n\nThe fit of the measurement model was assessed using the ratio of the Chi-square to the degrees of freedom, Tucker-Lewis Index (TLI), Comparative Fit Index (CFI), and the Root Mean Square Error of Approximation (RMSEA).\n\nFor the recommended values of the fit measures, see Table 3 below taken from the study of W. Shiau & M. Luo [-@shiau_luo].\n\n![](plots/sample_gof.png){fig-align=\"center\" width=50% height=50%}\n\n::: {style=\"font-size:70%; color=#6c757d;\"}\nSample GOF results from W. Shiau & M. Luo [-@shiau_luo]. Continuance intention of blog users: The impact of perceived enjoyment, habit, user involvement and blogging time\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Fitting CFA model\ncfa_fit <- cfa(model = cfa_model, \n               data = case_data_items, \n               estimator = \"MLR\")\n\n## Summary results\ncfa_fit %>% summary(standardized = TRUE,\n                     fit.measures = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlavaan 0.6.13 ended normally after 57 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        69\n\n  Number of observations                           272\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               546.100     483.499\n  Degrees of freedom                               309         309\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.129\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              5050.919    4114.132\n  Degrees of freedom                               351         351\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.228\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.950       0.954\n  Tucker-Lewis Index (TLI)                       0.943       0.947\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.957\n  Robust Tucker-Lewis Index (TLI)                            0.952\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -10679.787  -10679.787\n  Scaling correction factor                                  1.510\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -10406.737  -10406.737\n  Scaling correction factor                                  1.199\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               21497.575   21497.575\n  Bayesian (BIC)                             21746.375   21746.375\n  Sample-size adjusted Bayesian (SABIC)      21527.595   21527.595\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.053       0.046\n  90 Percent confidence interval - lower         0.046       0.038\n  90 Percent confidence interval - upper         0.060       0.053\n  P-value H_0: RMSEA <= 0.050                    0.236       0.839\n  P-value H_0: RMSEA >= 0.080                    0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.048\n  90 Percent confidence interval - lower                     0.040\n  90 Percent confidence interval - upper                     0.057\n  P-value H_0: Robust RMSEA <= 0.050                         0.615\n  P-value H_0: Robust RMSEA >= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.053       0.053\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                     Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  tangible =~                                                             \n    bt1                 1.000                               0.913    0.699\n    bt2                 1.022    0.081   12.634    0.000    0.932    0.696\n    bt4                 0.983    0.098   10.003    0.000    0.897    0.655\n    bt5                 1.070    0.102   10.542    0.000    0.977    0.691\n    bt6                 1.126    0.120    9.354    0.000    1.028    0.717\n    bt7                 1.138    0.128    8.890    0.000    1.039    0.694\n  drivers_quality =~                                                      \n    bd1                 1.000                               1.145    0.762\n    bd2                 0.942    0.071   13.188    0.000    1.079    0.762\n    bd3                 1.039    0.065   15.858    0.000    1.189    0.801\n    bd4                 0.897    0.068   13.217    0.000    1.027    0.767\n  empathy =~                                                              \n    emp1                1.000                               1.145    0.705\n    emp2                0.923    0.078   11.875    0.000    1.057    0.731\n    emp3                0.922    0.091   10.179    0.000    1.056    0.627\n    emp4                0.867    0.081   10.696    0.000    0.993    0.739\n    emp5                0.877    0.093    9.428    0.000    1.004    0.681\n  env_perf =~                                                             \n    ep1                 1.000                               1.260    0.909\n    ep2                 1.065    0.035   30.476    0.000    1.342    0.982\n    ep3                 0.851    0.061   13.892    0.000    1.072    0.774\n    ep4                 0.893    0.050   17.777    0.000    1.125    0.782\n  customer_sat =~                                                         \n    cs1                 1.000                               1.212    0.910\n    cs2                 1.022    0.040   25.743    0.000    1.239    0.955\n    cs3                 1.045    0.043   24.120    0.000    1.268    0.889\n  life_sat =~                                                             \n    ls1                 1.000                               1.058    0.855\n    ls2                 1.091    0.055   19.807    0.000    1.154    0.926\n    ls3                 1.051    0.096   10.991    0.000    1.112    0.851\n    ls4                 1.102    0.069   15.894    0.000    1.166    0.799\n    ls5                 0.876    0.090    9.678    0.000    0.927    0.597\n\nCovariances:\n                     Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  tangible ~~                                                             \n    drivers_qualty      0.721    0.100    7.196    0.000    0.690    0.690\n    empathy             0.492    0.080    6.144    0.000    0.471    0.471\n    env_perf            0.596    0.097    6.166    0.000    0.518    0.518\n    customer_sat        0.625    0.094    6.680    0.000    0.565    0.565\n    life_sat            0.353    0.072    4.895    0.000    0.366    0.366\n  drivers_quality ~~                                                      \n    empathy             0.798    0.112    7.142    0.000    0.609    0.609\n    env_perf            0.641    0.105    6.074    0.000    0.444    0.444\n    customer_sat        0.789    0.112    7.041    0.000    0.568    0.568\n    life_sat            0.344    0.088    3.923    0.000    0.284    0.284\n  empathy ~~                                                              \n    env_perf            0.596    0.099    6.000    0.000    0.413    0.413\n    customer_sat        0.831    0.114    7.278    0.000    0.599    0.599\n    life_sat            0.316    0.098    3.239    0.001    0.261    0.261\n  env_perf ~~                                                             \n    customer_sat        0.733    0.109    6.743    0.000    0.480    0.480\n    life_sat            0.368    0.094    3.919    0.000    0.276    0.276\n  customer_sat ~~                                                         \n    life_sat            0.385    0.077    5.022    0.000    0.300    0.300\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .bt1               0.870    0.126    6.918    0.000    0.870    0.511\n   .bt2               0.927    0.111    8.366    0.000    0.927    0.516\n   .bt4               1.070    0.127    8.427    0.000    1.070    0.571\n   .bt5               1.043    0.126    8.311    0.000    1.043    0.522\n   .bt6               0.999    0.112    8.904    0.000    0.999    0.486\n   .bt7               1.164    0.139    8.396    0.000    1.164    0.519\n   .bd1               0.946    0.114    8.271    0.000    0.946    0.419\n   .bd2               0.842    0.114    7.374    0.000    0.842    0.420\n   .bd3               0.791    0.103    7.659    0.000    0.791    0.359\n   .bd4               0.737    0.095    7.751    0.000    0.737    0.411\n   .emp1              1.330    0.146    9.121    0.000    1.330    0.504\n   .emp2              0.971    0.115    8.432    0.000    0.971    0.465\n   .emp3              1.722    0.180    9.574    0.000    1.722    0.607\n   .emp4              0.818    0.099    8.296    0.000    0.818    0.453\n   .emp5              1.163    0.142    8.186    0.000    1.163    0.536\n   .ep1               0.334    0.059    5.632    0.000    0.334    0.174\n   .ep2               0.068    0.043    1.566    0.117    0.068    0.036\n   .ep3               0.772    0.142    5.421    0.000    0.772    0.402\n   .ep4               0.803    0.152    5.299    0.000    0.803    0.388\n   .cs1               0.307    0.078    3.950    0.000    0.307    0.173\n   .cs2               0.148    0.041    3.631    0.000    0.148    0.088\n   .cs3               0.425    0.101    4.202    0.000    0.425    0.209\n   .ls1               0.411    0.078    5.276    0.000    0.411    0.269\n   .ls2               0.222    0.049    4.563    0.000    0.222    0.143\n   .ls3               0.472    0.125    3.786    0.000    0.472    0.276\n   .ls4               0.770    0.099    7.807    0.000    0.770    0.362\n   .ls5               1.548    0.180    8.579    0.000    1.548    0.643\n    tangible          0.833    0.134    6.237    0.000    1.000    1.000\n    drivers_qualty    1.311    0.175    7.494    0.000    1.000    1.000\n    empathy           1.311    0.188    6.980    0.000    1.000    1.000\n    env_perf          1.588    0.138   11.538    0.000    1.000    1.000\n    customer_sat      1.470    0.159    9.267    0.000    1.000    1.000\n    life_sat          1.120    0.149    7.519    0.000    1.000    1.000\n```\n:::\n\n```{.r .cell-code}\n## plotting cfa model\nlavaanPlot(model = cfa_fit, coefs = TRUE, covs = TRUE)\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"grViz html-widget html-fill-item-overflow-hidden html-fill-item\" id=\"htmlwidget-898dec583d7fd1c114ad\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-898dec583d7fd1c114ad\">{\"x\":{\"diagram\":\" digraph plot { \\n graph [ overlap = true, fontsize = 10 ] \\n node [ shape = box ] \\n node [shape = box] \\n bt1; bt2; bt4; bt5; bt6; bt7; bd1; bd2; bd3; bd4; emp1; emp2; emp3; emp4; emp5; ep1; ep2; ep3; ep4; cs1; cs2; cs3; ls1; ls2; ls3; ls4; ls5 \\n node [shape = oval] \\n tangible; drivers_quality; empathy; env_perf; customer_sat; life_sat \\n \\n edge [ color = black ] \\n  tangible->bt1 [label = \\\"1\\\"] tangible->bt2 [label = \\\"1.02\\\"] tangible->bt4 [label = \\\"0.98\\\"] tangible->bt5 [label = \\\"1.07\\\"] tangible->bt6 [label = \\\"1.13\\\"] tangible->bt7 [label = \\\"1.14\\\"] drivers_quality->bd1 [label = \\\"1\\\"] drivers_quality->bd2 [label = \\\"0.94\\\"] drivers_quality->bd3 [label = \\\"1.04\\\"] drivers_quality->bd4 [label = \\\"0.9\\\"] empathy->emp1 [label = \\\"1\\\"] empathy->emp2 [label = \\\"0.92\\\"] empathy->emp3 [label = \\\"0.92\\\"] empathy->emp4 [label = \\\"0.87\\\"] empathy->emp5 [label = \\\"0.88\\\"] env_perf->ep1 [label = \\\"1\\\"] env_perf->ep2 [label = \\\"1.06\\\"] env_perf->ep3 [label = \\\"0.85\\\"] env_perf->ep4 [label = \\\"0.89\\\"] customer_sat->cs1 [label = \\\"1\\\"] customer_sat->cs2 [label = \\\"1.02\\\"] customer_sat->cs3 [label = \\\"1.05\\\"] life_sat->ls1 [label = \\\"1\\\"] life_sat->ls2 [label = \\\"1.09\\\"] life_sat->ls3 [label = \\\"1.05\\\"] life_sat->ls4 [label = \\\"1.1\\\"] life_sat->ls5 [label = \\\"0.88\\\"] drivers_quality -> tangible [label = \\\"0.72\\\", dir = \\\"both\\\"] empathy -> tangible [label = \\\"0.49\\\", dir = \\\"both\\\"] env_perf -> tangible [label = \\\"0.6\\\", dir = \\\"both\\\"] customer_sat -> tangible [label = \\\"0.63\\\", dir = \\\"both\\\"] life_sat -> tangible [label = \\\"0.35\\\", dir = \\\"both\\\"] empathy -> drivers_quality [label = \\\"0.8\\\", dir = \\\"both\\\"] env_perf -> drivers_quality [label = \\\"0.64\\\", dir = \\\"both\\\"] customer_sat -> drivers_quality [label = \\\"0.79\\\", dir = \\\"both\\\"] life_sat -> drivers_quality [label = \\\"0.34\\\", dir = \\\"both\\\"] env_perf -> empathy [label = \\\"0.6\\\", dir = \\\"both\\\"] customer_sat -> empathy [label = \\\"0.83\\\", dir = \\\"both\\\"] life_sat -> empathy [label = \\\"0.32\\\", dir = \\\"both\\\"] customer_sat -> env_perf [label = \\\"0.73\\\", dir = \\\"both\\\"] life_sat -> env_perf [label = \\\"0.37\\\", dir = \\\"both\\\"] life_sat -> customer_sat [label = \\\"0.38\\\", dir = \\\"both\\\"]\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\n### Relibility and validity tests\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Reliability and validity \nreliability(cfa_fit) %>% round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       tangible drivers_quality empathy env_perf customer_sat life_sat\nalpha      0.85            0.86    0.82     0.92         0.94     0.90\nomega      0.85            0.86    0.82     0.92         0.94     0.90\nomega2     0.85            0.86    0.82     0.92         0.94     0.90\nomega3     0.85            0.86    0.82     0.92         0.94     0.89\navevar     0.48            0.60    0.48     0.75         0.84     0.63\n```\n:::\n:::\n\n\n## Structural equation modelling\n\n### Specifying structural model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Specifying structural model\nebus_model <- \"tangible =~ bt1 + bt2 + bt4 + bt5 + bt6 + bt7\n              drivers_quality =~ bd1 + bd2 + bd3 + bd4\n              empathy =~ emp1 + emp2 + emp3 + emp4 + emp5\n              env_perf =~ ep1 + ep2 + ep3 + ep4\n              customer_sat =~ cs1 + cs2 + cs3\n              life_sat =~ ls1 + ls2 + ls3 + ls4 + ls5\n              \n              # structural model\n              customer_sat ~ tangible + drivers_quality + empathy + env_perf\n              life_sat ~ customer_sat\"\n```\n:::\n\n\n### Fitting the structural model\n\nIn the previous section, we only defined our model. Here we fit our specified model using the `Lavaan` package [@rosseel_lavaan] for SEM analysis. The output composes mainly of the fit indices and parameter estimates. The parameter estimates are shown under the Latent variables, Regressions, and Variances section.\n\n**Latent variables**\n\nThe Estimates column is the non-standardized factor loadings coefficient accompanied by its Std Error (standard error). The most important to look at in the Estimates column is the presence of negative values since variance should not be negative. Thus, a negative value is a sign of a problem (see Heywood case for more information).\n\nFor interpretation, for example, the non-standardized factor loading of `bt2`, that is, `1.021`, indicates an increase of one unit in the latent variable, `tangible` factor, is associated with an increase of `1.021` in `bt2` item.\n\nThe last column std.lv and std.all present a standardized solution. The std.lv presents a factor loading of a solution that only latent variables (factors) were standardized. Whereas the std.all presents the factor loading of a solution where latent variables and indicators were standardized. We want a value to be positive and greater than 0.40 as a required minimum to evaluate the relevance of the indicators and the factor. However, this set required minimum is not unanimously accepted, but the larger, the better.\n\n\n... in progress.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Fitting structural model\nebus_fit <- sem(model = ebus_model,\n                data = case_data,\n                estimator = \"MLR\")\n\n## Summary results\nebus_fit %>% summary(standardized = TRUE,\n                     fit.measures = TRUE,\n                     rsq = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlavaan 0.6.13 ended normally after 46 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        65\n\n  Number of observations                           272\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               561.166     496.106\n  Degrees of freedom                               313         313\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.131\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              5050.919    4114.132\n  Degrees of freedom                               351         351\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.228\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.947       0.951\n  Tucker-Lewis Index (TLI)                       0.941       0.945\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.955\n  Robust Tucker-Lewis Index (TLI)                            0.950\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -10687.320  -10687.320\n  Scaling correction factor                                  1.525\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -10406.737  -10406.737\n  Scaling correction factor                                  1.199\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               21504.640   21504.640\n  Bayesian (BIC)                             21739.017   21739.017\n  Sample-size adjusted Bayesian (SABIC)      21532.920   21532.920\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.054       0.046\n  90 Percent confidence interval - lower         0.047       0.039\n  90 Percent confidence interval - upper         0.061       0.053\n  P-value H_0: RMSEA <= 0.050                    0.178       0.793\n  P-value H_0: RMSEA >= 0.080                    0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.049\n  90 Percent confidence interval - lower                     0.041\n  90 Percent confidence interval - upper                     0.057\n  P-value H_0: Robust RMSEA <= 0.050                         0.545\n  P-value H_0: Robust RMSEA >= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.069       0.069\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                     Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  tangible =~                                                             \n    bt1                 1.000                               0.912    0.699\n    bt2                 1.021    0.081   12.564    0.000    0.931    0.695\n    bt4                 0.986    0.098   10.030    0.000    0.900    0.657\n    bt5                 1.079    0.103   10.493    0.000    0.984    0.697\n    bt6                 1.124    0.121    9.299    0.000    1.026    0.716\n    bt7                 1.132    0.128    8.862    0.000    1.033    0.689\n  drivers_quality =~                                                      \n    bd1                 1.000                               1.145    0.762\n    bd2                 0.941    0.071   13.196    0.000    1.078    0.761\n    bd3                 1.038    0.065   15.982    0.000    1.189    0.801\n    bd4                 0.897    0.068   13.220    0.000    1.027    0.767\n  empathy =~                                                              \n    emp1                1.000                               1.144    0.704\n    emp2                0.922    0.078   11.823    0.000    1.055    0.730\n    emp3                0.923    0.091   10.159    0.000    1.056    0.627\n    emp4                0.870    0.080   10.805    0.000    0.995    0.741\n    emp5                0.878    0.093    9.403    0.000    1.004    0.681\n  env_perf =~                                                             \n    ep1                 1.000                               1.260    0.909\n    ep2                 1.065    0.035   30.266    0.000    1.342    0.982\n    ep3                 0.851    0.061   13.865    0.000    1.072    0.773\n    ep4                 0.893    0.050   17.778    0.000    1.125    0.782\n  customer_sat =~                                                         \n    cs1                 1.000                               1.212    0.909\n    cs2                 1.023    0.040   25.878    0.000    1.240    0.955\n    cs3                 1.045    0.043   24.227    0.000    1.266    0.888\n  life_sat =~                                                             \n    ls1                 1.000                               1.058    0.855\n    ls2                 1.092    0.056   19.402    0.000    1.156    0.927\n    ls3                 1.050    0.096   10.969    0.000    1.111    0.850\n    ls4                 1.102    0.070   15.713    0.000    1.166    0.799\n    ls5                 0.874    0.091    9.609    0.000    0.925    0.596\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  customer_sat ~                                                        \n    tangible          0.313    0.127    2.457    0.014    0.235    0.235\n    drivers_qualty    0.132    0.122    1.087    0.277    0.125    0.125\n    empathy           0.367    0.110    3.347    0.001    0.346    0.346\n    env_perf          0.155    0.060    2.604    0.009    0.162    0.162\n  life_sat ~                                                            \n    customer_sat      0.270    0.055    4.908    0.000    0.309    0.309\n\nCovariances:\n                     Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  tangible ~~                                                             \n    drivers_qualty      0.721    0.100    7.180    0.000    0.690    0.690\n    empathy             0.492    0.080    6.144    0.000    0.471    0.471\n    env_perf            0.595    0.097    6.132    0.000    0.517    0.517\n  drivers_quality ~~                                                      \n    empathy             0.798    0.112    7.139    0.000    0.609    0.609\n    env_perf            0.641    0.105    6.074    0.000    0.444    0.444\n  empathy ~~                                                              \n    env_perf            0.595    0.099    5.997    0.000    0.412    0.412\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .bt1               0.870    0.125    6.942    0.000    0.870    0.511\n   .bt2               0.929    0.111    8.372    0.000    0.929    0.517\n   .bt4               1.065    0.125    8.490    0.000    1.065    0.568\n   .bt5               1.029    0.124    8.300    0.000    1.029    0.515\n   .bt6               1.002    0.114    8.809    0.000    1.002    0.488\n   .bt7               1.177    0.139    8.464    0.000    1.177    0.525\n   .bd1               0.945    0.114    8.276    0.000    0.945    0.419\n   .bd2               0.843    0.114    7.376    0.000    0.843    0.420\n   .bd3               0.792    0.103    7.675    0.000    0.792    0.359\n   .bd4               0.738    0.095    7.748    0.000    0.738    0.411\n   .emp1              1.332    0.146    9.134    0.000    1.332    0.504\n   .emp2              0.975    0.116    8.420    0.000    0.975    0.467\n   .emp3              1.721    0.180    9.566    0.000    1.721    0.607\n   .emp4              0.814    0.098    8.350    0.000    0.814    0.451\n   .emp5              1.163    0.142    8.181    0.000    1.163    0.536\n   .ep1               0.335    0.059    5.646    0.000    0.335    0.174\n   .ep2               0.066    0.043    1.518    0.129    0.066    0.035\n   .ep3               0.773    0.143    5.417    0.000    0.773    0.402\n   .ep4               0.804    0.152    5.307    0.000    0.804    0.389\n   .cs1               0.307    0.077    3.971    0.000    0.307    0.173\n   .cs2               0.148    0.040    3.713    0.000    0.148    0.088\n   .cs3               0.428    0.100    4.262    0.000    0.428    0.211\n   .ls1               0.412    0.080    5.147    0.000    0.412    0.269\n   .ls2               0.219    0.049    4.503    0.000    0.219    0.141\n   .ls3               0.475    0.125    3.815    0.000    0.475    0.278\n   .ls4               0.769    0.098    7.818    0.000    0.769    0.361\n   .ls5               1.551    0.180    8.608    0.000    1.551    0.644\n    tangible          0.832    0.134    6.206    0.000    1.000    1.000\n    drivers_qualty    1.312    0.174    7.521    0.000    1.000    1.000\n    empathy           1.309    0.189    6.942    0.000    1.000    1.000\n    env_perf          1.587    0.138   11.533    0.000    1.000    1.000\n   .customer_sat      0.748    0.090    8.314    0.000    0.509    0.509\n   .life_sat          1.012    0.143    7.089    0.000    0.904    0.904\n\nR-Square:\n                   Estimate\n    bt1               0.489\n    bt2               0.483\n    bt4               0.432\n    bt5               0.485\n    bt6               0.512\n    bt7               0.475\n    bd1               0.581\n    bd2               0.580\n    bd3               0.641\n    bd4               0.589\n    emp1              0.496\n    emp2              0.533\n    emp3              0.393\n    emp4              0.549\n    emp5              0.464\n    ep1               0.826\n    ep2               0.965\n    ep3               0.598\n    ep4               0.611\n    cs1               0.827\n    cs2               0.912\n    cs3               0.789\n    ls1               0.731\n    ls2               0.859\n    ls3               0.722\n    ls4               0.639\n    ls5               0.356\n    customer_sat      0.491\n    life_sat          0.096\n```\n:::\n\n```{.r .cell-code}\n## plotting sem model using LavaanPlot package\nlavaanPlot(model = ebus_fit, coefs = TRUE, covs = TRUE, stars = TRUE, digits = 2)\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"grViz html-widget html-fill-item-overflow-hidden html-fill-item\" id=\"htmlwidget-e56385131d8aec9136f5\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-e56385131d8aec9136f5\">{\"x\":{\"diagram\":\" digraph plot { \\n graph [ overlap = true, fontsize = 10 ] \\n node [ shape = box ] \\n node [shape = box] \\n bt1; bt2; bt4; bt5; bt6; bt7; bd1; bd2; bd3; bd4; emp1; emp2; emp3; emp4; emp5; ep1; ep2; ep3; ep4; cs1; cs2; cs3; ls1; ls2; ls3; ls4; ls5 \\n node [shape = oval] \\n tangible; drivers_quality; empathy; env_perf; customer_sat; life_sat \\n \\n edge [ color = black ] \\n tangible->customer_sat [label = \\\"0.31\\\"] drivers_quality->customer_sat [label = \\\"0.13\\\"] empathy->customer_sat [label = \\\"0.37\\\"] env_perf->customer_sat [label = \\\"0.16\\\"] customer_sat->life_sat [label = \\\"0.27\\\"] tangible->bt1 [label = \\\"1\\\"] tangible->bt2 [label = \\\"1.02\\\"] tangible->bt4 [label = \\\"0.99\\\"] tangible->bt5 [label = \\\"1.08\\\"] tangible->bt6 [label = \\\"1.12\\\"] tangible->bt7 [label = \\\"1.13\\\"] drivers_quality->bd1 [label = \\\"1\\\"] drivers_quality->bd2 [label = \\\"0.94\\\"] drivers_quality->bd3 [label = \\\"1.04\\\"] drivers_quality->bd4 [label = \\\"0.9\\\"] empathy->emp1 [label = \\\"1\\\"] empathy->emp2 [label = \\\"0.92\\\"] empathy->emp3 [label = \\\"0.92\\\"] empathy->emp4 [label = \\\"0.87\\\"] empathy->emp5 [label = \\\"0.88\\\"] env_perf->ep1 [label = \\\"1\\\"] env_perf->ep2 [label = \\\"1.07\\\"] env_perf->ep3 [label = \\\"0.85\\\"] env_perf->ep4 [label = \\\"0.89\\\"] customer_sat->cs1 [label = \\\"1\\\"] customer_sat->cs2 [label = \\\"1.02\\\"] customer_sat->cs3 [label = \\\"1.04\\\"] life_sat->ls1 [label = \\\"1\\\"] life_sat->ls2 [label = \\\"1.09\\\"] life_sat->ls3 [label = \\\"1.05\\\"] life_sat->ls4 [label = \\\"1.1\\\"] life_sat->ls5 [label = \\\"0.87\\\"] drivers_quality -> tangible [label = \\\"0.72\\\", dir = \\\"both\\\"] empathy -> tangible [label = \\\"0.49\\\", dir = \\\"both\\\"] env_perf -> tangible [label = \\\"0.59\\\", dir = \\\"both\\\"] empathy -> drivers_quality [label = \\\"0.8\\\", dir = \\\"both\\\"] env_perf -> drivers_quality [label = \\\"0.64\\\", dir = \\\"both\\\"] env_perf -> empathy [label = \\\"0.59\\\", dir = \\\"both\\\"]\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n\n```{.r .cell-code}\n## plotting sem model using semPlot package\nsemPlot::semPaths(object = ebus_fit,\n                  what = \"std\",\n                  layout = \"tree\",\n                  rotation = 2,\n                  sizeMan = 3,\n                  sizeLat = 5,\n                  intercepts = FALSE,\n                  residuals = FALSE,\n                  exoCov = F)\n```\n\n::: {.cell-output-display}\n![](06-cb-sem-sample_files/figure-html/unnamed-chunk-11-2.png){width=672}\n:::\n:::\n\n\n### Estimating indirect effects\n\n#### Specifying model with indirect effects\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### SEM model with mediation\nebus_model_ie <- \"tangible =~ bt1 + bt2 + bt4 + bt5 + bt6 + bt7\n              drivers_quality =~ bd1 + bd2 + bd3 + bd4\n              empathy =~ emp1 + emp2 + emp3 + emp4 + emp5\n              env_perf =~ ep1 + ep2 + ep3 + ep4\n              customer_sat =~ cs1 + cs2 + cs3\n              life_sat =~ ls1 + ls2 + ls3 + ls4 + ls5\n              \n              # structural model\n              customer_sat ~ a*tangible + b*drivers_quality + c*empathy + d*env_perf\n              life_sat ~ e*customer_sat\n             \n              # indirect effects\n              ie_tangible := a*e\n              ie_drivers_qual := b*e\n              ie_empathy := c*e\n              ie_en_perf := d*e\"\n```\n:::\n\n\n#### Fitting the model with indirect effects\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Fitting structural model with mediation \nebus_fit_ie <- sem(model = ebus_model_ie,\n                   data = case_data,\n                   estimator = \"MLR\")\n\n### Summary results\nebus_fit_ie %>% summary(standardized = TRUE,\n                        fit.measures = TRUE,\n                        rsq = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlavaan 0.6.13 ended normally after 46 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        65\n\n  Number of observations                           272\n\nModel Test User Model:\n                                              Standard      Scaled\n  Test Statistic                               561.166     496.106\n  Degrees of freedom                               313         313\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  1.131\n    Yuan-Bentler correction (Mplus variant)                       \n\nModel Test Baseline Model:\n\n  Test statistic                              5050.919    4114.132\n  Degrees of freedom                               351         351\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.228\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.947       0.951\n  Tucker-Lewis Index (TLI)                       0.941       0.945\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.955\n  Robust Tucker-Lewis Index (TLI)                            0.950\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -10687.320  -10687.320\n  Scaling correction factor                                  1.525\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)     -10406.737  -10406.737\n  Scaling correction factor                                  1.199\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               21504.640   21504.640\n  Bayesian (BIC)                             21739.017   21739.017\n  Sample-size adjusted Bayesian (SABIC)      21532.920   21532.920\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.054       0.046\n  90 Percent confidence interval - lower         0.047       0.039\n  90 Percent confidence interval - upper         0.061       0.053\n  P-value H_0: RMSEA <= 0.050                    0.178       0.793\n  P-value H_0: RMSEA >= 0.080                    0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.049\n  90 Percent confidence interval - lower                     0.041\n  90 Percent confidence interval - upper                     0.057\n  P-value H_0: Robust RMSEA <= 0.050                         0.545\n  P-value H_0: Robust RMSEA >= 0.080                         0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.069       0.069\n\nParameter Estimates:\n\n  Standard errors                             Sandwich\n  Information bread                           Observed\n  Observed information based on                Hessian\n\nLatent Variables:\n                     Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  tangible =~                                                             \n    bt1                 1.000                               0.912    0.699\n    bt2                 1.021    0.081   12.564    0.000    0.931    0.695\n    bt4                 0.986    0.098   10.030    0.000    0.900    0.657\n    bt5                 1.079    0.103   10.493    0.000    0.984    0.697\n    bt6                 1.124    0.121    9.299    0.000    1.026    0.716\n    bt7                 1.132    0.128    8.862    0.000    1.033    0.689\n  drivers_quality =~                                                      \n    bd1                 1.000                               1.145    0.762\n    bd2                 0.941    0.071   13.196    0.000    1.078    0.761\n    bd3                 1.038    0.065   15.982    0.000    1.189    0.801\n    bd4                 0.897    0.068   13.220    0.000    1.027    0.767\n  empathy =~                                                              \n    emp1                1.000                               1.144    0.704\n    emp2                0.922    0.078   11.823    0.000    1.055    0.730\n    emp3                0.923    0.091   10.159    0.000    1.056    0.627\n    emp4                0.870    0.080   10.805    0.000    0.995    0.741\n    emp5                0.878    0.093    9.403    0.000    1.004    0.681\n  env_perf =~                                                             \n    ep1                 1.000                               1.260    0.909\n    ep2                 1.065    0.035   30.266    0.000    1.342    0.982\n    ep3                 0.851    0.061   13.865    0.000    1.072    0.773\n    ep4                 0.893    0.050   17.778    0.000    1.125    0.782\n  customer_sat =~                                                         \n    cs1                 1.000                               1.212    0.909\n    cs2                 1.023    0.040   25.878    0.000    1.240    0.955\n    cs3                 1.045    0.043   24.227    0.000    1.266    0.888\n  life_sat =~                                                             \n    ls1                 1.000                               1.058    0.855\n    ls2                 1.092    0.056   19.402    0.000    1.156    0.927\n    ls3                 1.050    0.096   10.969    0.000    1.111    0.850\n    ls4                 1.102    0.070   15.713    0.000    1.166    0.799\n    ls5                 0.874    0.091    9.609    0.000    0.925    0.596\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  customer_sat ~                                                        \n    tangible   (a)    0.313    0.127    2.457    0.014    0.235    0.235\n    drvrs_qlty (b)    0.132    0.122    1.087    0.277    0.125    0.125\n    empathy    (c)    0.367    0.110    3.347    0.001    0.346    0.346\n    env_perf   (d)    0.155    0.060    2.604    0.009    0.162    0.162\n  life_sat ~                                                            \n    customr_st (e)    0.270    0.055    4.908    0.000    0.309    0.309\n\nCovariances:\n                     Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  tangible ~~                                                             \n    drivers_qualty      0.721    0.100    7.180    0.000    0.690    0.690\n    empathy             0.492    0.080    6.144    0.000    0.471    0.471\n    env_perf            0.595    0.097    6.132    0.000    0.517    0.517\n  drivers_quality ~~                                                      \n    empathy             0.798    0.112    7.139    0.000    0.609    0.609\n    env_perf            0.641    0.105    6.074    0.000    0.444    0.444\n  empathy ~~                                                              \n    env_perf            0.595    0.099    5.997    0.000    0.412    0.412\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   .bt1               0.870    0.125    6.942    0.000    0.870    0.511\n   .bt2               0.929    0.111    8.372    0.000    0.929    0.517\n   .bt4               1.065    0.125    8.490    0.000    1.065    0.568\n   .bt5               1.029    0.124    8.300    0.000    1.029    0.515\n   .bt6               1.002    0.114    8.809    0.000    1.002    0.488\n   .bt7               1.177    0.139    8.464    0.000    1.177    0.525\n   .bd1               0.945    0.114    8.276    0.000    0.945    0.419\n   .bd2               0.843    0.114    7.376    0.000    0.843    0.420\n   .bd3               0.792    0.103    7.675    0.000    0.792    0.359\n   .bd4               0.738    0.095    7.748    0.000    0.738    0.411\n   .emp1              1.332    0.146    9.134    0.000    1.332    0.504\n   .emp2              0.975    0.116    8.420    0.000    0.975    0.467\n   .emp3              1.721    0.180    9.566    0.000    1.721    0.607\n   .emp4              0.814    0.098    8.350    0.000    0.814    0.451\n   .emp5              1.163    0.142    8.181    0.000    1.163    0.536\n   .ep1               0.335    0.059    5.646    0.000    0.335    0.174\n   .ep2               0.066    0.043    1.518    0.129    0.066    0.035\n   .ep3               0.773    0.143    5.417    0.000    0.773    0.402\n   .ep4               0.804    0.152    5.307    0.000    0.804    0.389\n   .cs1               0.307    0.077    3.971    0.000    0.307    0.173\n   .cs2               0.148    0.040    3.713    0.000    0.148    0.088\n   .cs3               0.428    0.100    4.262    0.000    0.428    0.211\n   .ls1               0.412    0.080    5.147    0.000    0.412    0.269\n   .ls2               0.219    0.049    4.503    0.000    0.219    0.141\n   .ls3               0.475    0.125    3.815    0.000    0.475    0.278\n   .ls4               0.769    0.098    7.818    0.000    0.769    0.361\n   .ls5               1.551    0.180    8.608    0.000    1.551    0.644\n    tangible          0.832    0.134    6.206    0.000    1.000    1.000\n    drivers_qualty    1.312    0.174    7.521    0.000    1.000    1.000\n    empathy           1.309    0.189    6.942    0.000    1.000    1.000\n    env_perf          1.587    0.138   11.533    0.000    1.000    1.000\n   .customer_sat      0.748    0.090    8.314    0.000    0.509    0.509\n   .life_sat          1.012    0.143    7.089    0.000    0.904    0.904\n\nR-Square:\n                   Estimate\n    bt1               0.489\n    bt2               0.483\n    bt4               0.432\n    bt5               0.485\n    bt6               0.512\n    bt7               0.475\n    bd1               0.581\n    bd2               0.580\n    bd3               0.641\n    bd4               0.589\n    emp1              0.496\n    emp2              0.533\n    emp3              0.393\n    emp4              0.549\n    emp5              0.464\n    ep1               0.826\n    ep2               0.965\n    ep3               0.598\n    ep4               0.611\n    cs1               0.827\n    cs2               0.912\n    cs3               0.789\n    ls1               0.731\n    ls2               0.859\n    ls3               0.722\n    ls4               0.639\n    ls5               0.356\n    customer_sat      0.491\n    life_sat          0.096\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n    ie_tangible       0.084    0.039    2.144    0.032    0.073    0.073\n    ie_drivers_qul    0.036    0.034    1.058    0.290    0.039    0.039\n    ie_empathy        0.099    0.032    3.051    0.002    0.107    0.107\n    ie_en_perf        0.042    0.018    2.292    0.022    0.050    0.050\n```\n:::\n:::\n",
    "supporting": [
      "06-cb-sem-sample_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/htmlwidgets-1.6.1/htmlwidgets.js\"></script>\r\n<link href=\"site_libs/datatables-css-0.0.0/datatables-crosstalk.css\" rel=\"stylesheet\" />\r\n<script src=\"site_libs/datatables-binding-0.27/datatables.js\"></script>\r\n<script src=\"site_libs/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\r\n<link href=\"site_libs/dt-core-1.12.1/css/jquery.dataTables.min.css\" rel=\"stylesheet\" />\r\n<link href=\"site_libs/dt-core-1.12.1/css/jquery.dataTables.extra.css\" rel=\"stylesheet\" />\r\n<script src=\"site_libs/dt-core-1.12.1/js/jquery.dataTables.min.js\"></script>\r\n<link href=\"site_libs/crosstalk-1.2.0/css/crosstalk.min.css\" rel=\"stylesheet\" />\r\n<script src=\"site_libs/crosstalk-1.2.0/js/crosstalk.min.js\"></script>\r\n<script src=\"site_libs/viz-1.8.2/viz.js\"></script>\r\n<link href=\"site_libs/DiagrammeR-styles-0.2/styles.css\" rel=\"stylesheet\" />\r\n<script src=\"site_libs/grViz-binding-1.0.9/grViz.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}